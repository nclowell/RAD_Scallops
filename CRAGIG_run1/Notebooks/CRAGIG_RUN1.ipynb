{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CRAGIG RUN 1\n",
    "20170202 \n",
    "\n",
    "I got my first batch of RAD data back from BGI, and have yet to really look at it. I'd like to take a first look for population differences, but also I need to help Molly figure out how to build a parentage panel. My first look will be at the **forward reads.**\n",
    "\n",
    "### Download Data from BGI\n",
    "\n",
    "I downloaded the data by clicking on the download FTP button. Downloading each of the two raw data files took about 24 hours, and crashed occasionally. I then moved the two files onto the E drive (which has a ton of space), copied them, and saved on as untouchable. The other, I put in a shared folder so I can play with it in my virtual machine.\n",
    "\n",
    "### Check that Download was Error Free\n",
    "\n",
    "used md5sum, a built in function in Ubuntu, and compared the check sum associated with the raw forward read file and it matched\n",
    "\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder$ md5sum 161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_1.fq.gz\n",
    "\n",
    "6d7714b6f03d65002894ba5d1c16b520  161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_1.fq.gz\n",
    "```\n",
    "\n",
    "While I'm' at it, going to check the reverse reads too:\n",
    "\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/Raw_data$ md5sum 161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_2.fq.gz \n",
    "27ed5a022a51bbc5fa23300bb22e602e  161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_2.fq.gz\n",
    "```\n",
    "\n",
    "### ``process_radtags``\n",
    "\n",
    "``process_radtags`` dumultiplexes your raw data file and (optionally) names the sample by file name. Be sure to include unique file names for any replicates. For example, fg100-fg104, I sequenced twice, each with a unique barcode.\n",
    "\n",
    "\n",
    "[mary says don't use filterillumina; used r c q and trimmed]\n",
    "\n",
    "Stacks [manual page for ``process_radtags``](http://catchenlab.life.illinois.edu/stacks/comp/process_radtags.php)\n",
    "\n",
    "Inputs values I will use\n",
    "\n",
    " f — path to the input file if processing single-end seqeunces \n",
    "<br>**-f /mnt/hgfs/Data_for Analysis/WorkingFolder/161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_1.fq.gz**\n",
    "\n",
    " i — input file type, either 'bustard' for the Illumina BUSTARD format, 'bam', 'fastq' (default), or 'gzfastq' for gzipped FASTQ.\n",
    "<br>**-i gzfastq**\n",
    "\n",
    " y — output type, either 'fastq', 'gzfastq', 'fasta', or 'gzfasta' (default is to match the input file type).\n",
    "<br>**-y gzfastq**\n",
    "\n",
    " o — path to output the processed files.\n",
    "<br>**-o /mnt/hgfs/Data_for Analysis/WorkingFolder/Stacks**\n",
    "\n",
    " t — truncate final read length to this value.\n",
    "<br>**-t 139** (reads are 150 bp, 6 of which are barcode, and trim at least 5, so start with 139. Then run FastQC and see if more trimming is necessary)\n",
    "\n",
    " b — path to a file containing barcodes for this run.\n",
    "<br>**-b /mnt/hgfs/Data_for Analysis/WorkingFolder/barcodes_cragigrun1.txt**\n",
    "\n",
    " r — rescue barcodes and RAD-Tags (this means if barcode is one base off from any barcode you used, Stacks will use some probability function to match to closest barcode and rewrite sequence to reflect that)\n",
    "<br>**-r**\n",
    "\n",
    " c — clean data, remove any read with an uncalled base.\n",
    "<br>**-c**\n",
    "\n",
    " -e [enz], --renz_1 [enz]: provide the restriction enzyme used (cut site occurs on single-end read) <- 'sbfI'\n",
    "<br>**-e sbfI**\n",
    "\n",
    "Your barcode list:\n",
    "\n",
    "Should take the form of barcode \\t samplename \\n\n",
    "\n",
    "Local path for my current barcode list = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Change to appropriate working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Data_for Analysis/WorkingFolder\n"
     ]
    }
   ],
   "source": [
    "cd /mnt/hgfs/Data_for Analysis/WorkingFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!stacks process_radtags \\\n",
    "-f /mnt/hgfs/Data_for_Analysis/WorkingFolder/161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_1.fq.gz \\\n",
    "-i gzfastq \\\n",
    "-y gzfastq \\\n",
    "-o /mnt/hgfs/Data_for_Analysis/WorkingFolder/Stacks \\\n",
    "-t 139 \\\n",
    "-b /mnt/hgfs/Data_for_Analysis/WorkingFolder/barcodes_cragigrun1.txt \\\n",
    "-r \\\n",
    "-c \\\n",
    "-e sbfI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**``process_radtags`` output:**\n",
    "\n",
    "387289855 total sequences;\n",
    "  64147742 ambiguous barcode drops;\n",
    "  401015 low quality read drops;\n",
    "  42895231 ambiguous RAD-Tag drops;\n",
    "279845867 retained reads.\n",
    "\n",
    "So 72.26% reads retained.\n",
    "\n",
    "Then, I ran a script to count reads in each post-``process_radtags`` file, which Steven gave to Mary who gave to me. Here's the script. A similar version is on her github, [here](https://github.com/mfisher5/mf-fish546-PCod/blob/master/scripts/L1L2_seqCountsgen.py).\n",
    "\n",
    "```\n",
    "import sys\n",
    "lane1 = open(sys.argv[1], \"r\")\n",
    "newshell = open(\"CountFASTQseqs.sh\", \"w\")\n",
    "\n",
    "newshell.write(\"#!/bin/bash\" + \"\\n\\n\")\n",
    "\n",
    "for line in lane1:\n",
    "\tlinelist = line.strip().split()\n",
    "\tfilestring = \"zcat L1L2samples/\" + linelist[0] + \".fq.gz | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' >> FastQsequenceCounts.txt\"\n",
    "\tnewshell.write(filestring + \"\\n\")\n",
    "lane1.close()\n",
    "newshell.close()\n",
    "```\n",
    "\n",
    "I'm still working on getting ``matplotlib`` installed in my virtual machine, but until then, I exported the results to excel, sorted, and picked the ten samples with the most reads out of each population. For Alaska"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Quality check with FastQC\n",
    "\n",
    "Looked at several individual sample FastQC reports and they all look great. Mean average quality score of 40, with little variation, and none dropped below the green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ``ustacks``\n",
    "\n",
    "I used a python script I wrote previously to build the shell script. There was an error where it skipped the first few of my samples and didn't run ``ustacks`` on them, so I need to improve it. Also it would be great to be able to input the different parameters with flags. And my brother showed me a module that helps you do that... so I'm going to try to do that now.\n",
    "\n",
    "Example command to show parameter values:\n",
    "\n",
    "``stacks ustacks -t gzfastq -f ./Stacks/Q334.fq.gz -r -d -o ./Stacks -i 001 -m 10 -M 3 -p 10``\n",
    "\n",
    "\n",
    "Only one sample retained few loci (FG100_A retained only 6) so I'm going to exclude that one from further analyses. All of the rest of the samples had at least 20K, with the vast majority around 25K-30K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pick samples for ``cstacks``\n",
    "\n",
    "I just got clarified from lab mates that we're supposed to use the 10 most sequenced individuals from **each** population. I don't even have 10 from each in this run, so I'll include up to 10 from each population.\n",
    "\n",
    "Sorting my Excel sheet to determine these...\n",
    "\n",
    "These leads me to:\n",
    "Q324\n",
    "Q318\n",
    "Q326\n",
    "Q314\n",
    "Q316\n",
    "Q323\n",
    "Q325\n",
    "Q351\n",
    "Q352\n",
    "Q332\n",
    "Q339\n",
    "Q334\n",
    "Q347\n",
    "Q356\n",
    "Q330\n",
    "Q355\n",
    "Q354\n",
    "FG009\n",
    "FG033\n",
    "FG002\n",
    "FG027\n",
    "FG021\n",
    "FG013\n",
    "FG001\n",
    "FG005\n",
    "FG014\n",
    "FG006\n",
    "FG101_A\n",
    "FG102_A\n",
    "FG100_B\n",
    "FG106\n",
    "FG107\n",
    "FG109\n",
    "FG104_A\n",
    "FG111\n",
    "FG108\n",
    "FG103_A\n",
    "FG210\n",
    "FG205\n",
    "FG211\n",
    "FG212\n",
    "FG201\n",
    "FG209\n",
    "FG206\n",
    "FG204\n",
    "FG202\n",
    "FG203\n",
    "\n",
    "\n",
    "<br><br> There are 47 because the AK population only had 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ``cstacks``\n",
    "\n",
    "\n",
    "Now I need to write the ``cstacks`` shell script to run the program on these ten samples. I used my ``easy_cstacks.py`` script.\n",
    "\n",
    "Code for ``cstacks`` was:\n",
    "\n",
    "```\n",
    "stacks cstacks -b 100 -s Stacks/Q324 -s Stacks/Q318 -s Stacks/Q326 -s Stacks/Q314 -s Stacks/Q316 -s Stacks/Q323 -s Stacks/Q325 -s Stacks/Q351 -s Stacks/Q352 -s Stacks/Q332 -s Stacks/Q339 -s Stacks/Q334 -s Stacks/Q347 -s Stacks/Q356 -s Stacks/Q330 -s Stacks/Q355 -s Stacks/Q354 -s Stacks/FG009 -s Stacks/FG033 -s Stacks/FG002 -s Stacks/FG027 -s Stacks/FG021 -s Stacks/FG013 -s Stacks/FG001 -s Stacks/FG005 -s Stacks/FG014 -s Stacks/FG006 -s Stacks/FG101_A -s Stacks/FG102_A -s Stacks/FG100_B -s Stacks/FG106 -s Stacks/FG107 -s Stacks/FG109 -s Stacks/FG104_A -s Stacks/FG111 -s Stacks/FG108 -s Stacks/FG103_A -s Stacks/FG210 -s Stacks/FG205 -s Stacks/FG211 -s Stacks/FG212 -s Stacks/FG201 -s Stacks/FG209 -s Stacks/FG206 -s Stacks/FG204 -s Stacks/FG202 -s Stacks/FG203 -o Stacks -n 3 -p 3 \n",
    "\n",
    "```\n",
    "Worked! And it took ~6.5 hours to run.\n",
    "\n",
    "```\n",
    "Writing catalog to 'Stacks/... done.\n",
    "\n",
    "Running cstacks took \n",
    "06:27:18.40\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ``sstacks``\n",
    "\n",
    "Need to get all the way through populations in order to form catalog of loci for Bowtie and Blast filtering that will then feed back into the Stacks pipeline starting at ``pstacks``.\n",
    "\n",
    "I wrote a script that makes the ``sstacks`` shell, and runs it. Going to update it and run it. \n",
    "\n",
    "The shell script looks like this:\n",
    "\n",
    "```\n",
    "stacks sstacks -b 100 -s Stacks/Q351 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q352 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q332 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q339 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q334 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q347 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q356 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q330 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q355 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q354 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q338 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q353 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q324 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q318 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q326 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q314 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q316 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q323 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q325 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG009 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG033 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG002 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG027 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG021 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG013 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG001 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG005 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG014 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG006 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG004 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG024 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG017 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG003 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG018 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG019 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG016 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG015 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG031 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG032 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG007 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG026 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG011 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG035 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG034 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG010 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG101_A -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG102_A -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG100_B -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG106 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG101_B -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG107 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG109 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG102_B -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG104_A -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG111 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG108 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG103_A -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG103_B -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG104_B -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG100_A -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG210 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG205 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG211 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG212 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG201 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG209 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG206 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG204 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG202 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG203 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG207 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "\n",
    "```\n",
    "\n",
    "It worked! And it took 00:13:10.65 to run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Populations\n",
    "\n",
    "#### Question: \n",
    "How should my parameter sets compare between the first round of ``populations`` (to build reference genome) and the second round of ``populations`` when I'm actually comparing populations for popgen?\n",
    "\n",
    "Just talked to Dan, who recommended that I be pretty lenient the first time through, and then very stringent (for Molly's parentage panel needs) the second time through. For lenient, he said an r of 0.25 and for p, about half of the number of populations.\n",
    "\n",
    "Code for this run:\n",
    "\n",
    "```\n",
    "stacks populations -b 100 -P Stacks -M popmap_cragigrun1.txt -t 10 -r 0.25 -p 3 -m 5 --genepop\n",
    "```\n",
    "\n",
    "<br>Here:\n",
    "<br> -b 100 (batch num)\n",
    "<br> -P Stacks (directory w sstacks output files)\n",
    "<br> -M popmap_cragigrun1.txt (pop map)\n",
    "<br> -t 10 (threads)\n",
    "<br> -r .25 (percentage ind in pop required to process locus for pop)\n",
    "<br> -p 3 (min populations loci present in to keep locus)\n",
    "<br> -m 5 specify a minimum stack depth required for individuals at a locus.\n",
    "\n",
    "Output\n",
    "```\n",
    "Writing 28647 loci to summary statistics file, 'Stacks/batch_100.sumstats.tsv'\n",
    "Writing 28647 loci to observed haplotype file, 'Stacks/batch_100.haplotypes.tsv'\n",
    "Writing population data to GenePop file 'Stacks/batch_100.genepop'\n",
    "```\n",
    "\n",
    "Counted loci by columns in pop gen file - 1 (for first column of sample names):\n",
    "\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder$ python count_loci_genepop.py Stacks/batch_100.genepop\n",
    "Your genepop file has 232763 columns of genotypes.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bowtie + BLAST filtering\n",
    "\n",
    "6. Filtering loci with bowtie + BLAST\n",
    "\n",
    "We filter out highly repetive loci using bowtie and BLAST. More information in this markdown file. \n",
    "\n",
    "#### A. Filtering with bowtie\n",
    "Make a fasta file for bowtie by running a custom python script that Mary wrote, which requires (1) a text file with the header from the genepop file and (2) unzipped batch.catalog.tags file.\n",
    "\n",
    "Here, I will make batch_100_loci.txt for this round by manually opening the genepop file in Textwrangler, and cutting and pasting the header line with the tag names into its own text file.\n",
    "\n",
    "first bit of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_11,3_19,3_28,3_38,3_50,3_103,3_130,4_7,4_14,4_39,4_41,4_44,4_46,4_58,4_59,4_61,4_62,4_69,4_76,4_77,4_78,4_85,4_90,4_133,7_20,7_105,7_130,7_131,9_20,9_26,9_36,9_53,9_67,9_70,9_79,9_101,9_117,9_119,9_125,9_137,9_138,10_32,11_96,11_131,12_32,12_50,12_73,12_96,12_98,12_117,12_138,14_8,14_15,14_23,14_29,14_38,14_43,14_44,14_49,14_65,14_67,14_107,16_16,16_46,16_50,16_54,16_73,16_77,16_79,16_97,16_108,16_109,17_29,17_30,17_37,17_48,17_50,17_85,17_89,17_97,17_108,17_113,17_120,18_9,18_13,18_35,18_46,18_53,18_59,18_65,18_70,18_71,18_90,18_91,18_95,18_97,18_99,18_108,18_112,18_114,18_117,18_118,18_119,18_136,23_58,23_118,23_119,23_120,26_49,26_55,26_89,26_90,26_137,27_8,27_28,27_49,27_67,27_76,27_86,27_134,27_135,32_22,32_23,32_35,32_39,32_52,32_75,32_84,32_101,32_120,32_121,32_136,32_137,33_9,33_13,33_33,33_52,33_127,36_16,36_18,36_33,36_45,36_58,36_94,36_116,36_126,36_133,37_10,37_23,37_34,37_67,37_82,37_85,37_109,37_111,37_117,37_118,38_18,38_29,38_32,38_50,43_8,43_21,43_27,43_30,43_32,43_36,43_61,43_74,45_67,46_24,46_41,46_70,46_85,46_96,46_116,46_133,46_134,46_137,50_8,50_34,50_132,52_30,52_37,52_102,52_105,53_7,53_10,53_40,53_42,53_55,53_67,53_70,53_77,53_88,53_91,53_93,53_105,53_109,53_116,53_123,53_138,57_14,57_18,57_24,57_27,57_37,57_38,57_40,57_42,57_45,57_56,57_58,57_63,57_68,57_74,57_76,57_77,57_84,57_85,57_91,57_95,57_101,57_103,57_111,57_119,57_120,57_121,57_131,57_135,57_136,58_21,58_70,58_92,58_112,58_113,58_135,58_136,58_137,62_15,62_39,62_61,62_62,62_75,62_76,64_24,64_50,64_75,64_82,64_98,64_105,64_121,64_136,64_138,66_7,66_34,66_128,67_48,67_56,67_84,67_86,67_88,67_92,67_95,67_116,67_130,67_138,68_29,68_30,68_52,68_96,68_127,71_7,71_12,71_22,71_23,71_49,71_72,71_77,71_80,71_88,71_97,71_107,71_124,72_9,72_10,72_32,72_41,72_57,72_68,72_79,72_81,72_98,72_111,72_124,76_10,76_95,79_30,79_33,79_119,79_135,82_81,82_99,83_34,83_84,83_103,83_125,94_32,94_40,94_52,94_56,94_60,94_72,94_73,94_75,94_102,94_106,94_133,94_134,95_28,95_45,95_85,95_131,95_133,98_22,98_24,\r\n"
     ]
    }
   ],
   "source": [
    "!python get_first_line_genepop.py batch_100 batch_100_loci_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then I will unzip the catalog tags file with ``gzip -d batch_100.catalog.tags.tsv.gz``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Data_for_Analysis/WorkingFolder/Stacks\n"
     ]
    }
   ],
   "source": [
    "cd Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!gzip -d batch_100.catalog.tags.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And now Mary's script..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python gen_bowtie_fasta.py batch_100_loci_names.txt Stacks/batch_100.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Installed bowtie by downloading the zipped folder, navigating to that folder, and running\n",
    "\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder/Bowtie/bowtie-1.2$ sudo apt install bowtie\n",
    "[sudo] password for nclowell: \n",
    "```\n",
    "\n",
    "Manually made a new folder to hold the bowtie software and files I make with bowtie.\n",
    "\n",
    "Now, make bowtie index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "  Output files: \"batch_100.*.ebwt\"\n",
      "  Line rate: 6 (line is 64 bytes)\n",
      "  Lines per side: 1 (side is 64 bytes)\n",
      "  Offset rate: 5 (one in 32)\n",
      "  FTable chars: 10\n",
      "  Strings: unpacked\n",
      "  Max bucket size: default\n",
      "  Max bucket size, sqrt multiplier: default\n",
      "  Max bucket size, len divisor: 4\n",
      "  Difference-cover sample period: 1024\n",
      "  Endianness: little\n",
      "  Actual local endianness: little\n",
      "  Sanity checking: disabled\n",
      "  Assertions: disabled\n",
      "  Random seed: 0\n",
      "  Sizeofs: void*:8, int:4, long:8, size_t:8\n",
      "Input files DNA, FASTA:\n",
      "  seqsforBOWTIE.fa\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 954860\n",
      "Using parameters --bmax 716145 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 716145 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 6; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 477429 (target: 716144)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:01\n",
      "  Sorting block of length 587728\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 587729\n",
      "Getting block 2 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 430345\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 430346\n",
      "Getting block 3 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 291143\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 291144\n",
      "Getting block 4 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 637647\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 637648\n",
      "Getting block 5 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 637536\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 637537\n",
      "Getting block 6 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 477289\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:01\n",
      "Returning block of 477290\n",
      "Getting block 7 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 546629\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 546630\n",
      "Getting block 8 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 211118\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 211119\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 1123971\n",
      "fchr[G]: 1863730\n",
      "fchr[T]: 2618412\n",
      "fchr[$]: 3819442\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 5885248 bytes to primary EBWT file: batch_100.1.ebwt\n",
      "Wrote 477436 bytes to secondary EBWT file: batch_100.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 3819442\n",
      "    bwtLen: 3819443\n",
      "    sz: 954861\n",
      "    bwtSz: 954861\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 119358\n",
      "    offsSz: 477432\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 8526\n",
      "    numSides: 17052\n",
      "    numLines: 17052\n",
      "    ebwtTotLen: 1091328\n",
      "    ebwtTotSz: 1091328\n",
      "    reverse: 0\n",
      "Total time for call to driver() for forward index: 00:00:04\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 954860\n",
      "Using parameters --bmax 716145 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 716145 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:01\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 2, merged 6; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 545634 (target: 716144)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 432195\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 432196\n",
      "Getting block 2 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 527983\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:01\n",
      "Returning block of 527984\n",
      "Getting block 3 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 303252\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 303253\n",
      "Getting block 4 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 653457\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 653458\n",
      "Getting block 5 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 683581\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 683582\n",
      "Getting block 6 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 647813\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:01\n",
      "Returning block of 647814\n",
      "Getting block 7 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 571155\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 571156\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 1123971\n",
      "fchr[G]: 1863730\n",
      "fchr[T]: 2618412\n",
      "fchr[$]: 3819442\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 5885248 bytes to primary EBWT file: batch_100.rev.1.ebwt\n",
      "Wrote 477436 bytes to secondary EBWT file: batch_100.rev.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 3819442\n",
      "    bwtLen: 3819443\n",
      "    sz: 954861\n",
      "    bwtSz: 954861\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 119358\n",
      "    offsSz: 477432\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 8526\n",
      "    numSides: 17052\n",
      "    numLines: 17052\n",
      "    ebwtTotLen: 1091328\n",
      "    ebwtTotSz: 1091328\n",
      "    reverse: 0\n",
      "Total time for backward call to driver() for mirror index: 00:00:05\n"
     ]
    }
   ],
   "source": [
    "!bowtie-build seqsforBOWTIE.fa batch_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now align sequenecs to the index you just made to identify any sequences that align to multiple places in the genome (maybe highly repetive loci like microsats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reads processed: 27478\r\n",
      "# reads with at least one reported alignment: 27478 (100.00%)\r\n",
      "# reads that failed to align: 0 (0.00%)\r\n",
      "Reported 27478 alignments to 1 output stream(s)\r\n"
     ]
    }
   ],
   "source": [
    "!bowtie -f -v 3 --sam --sam-nohead \\\n",
    "batch_100 \\\n",
    "seqsforBOWTIE.fa \\\n",
    "batch_100_BOWTIEout.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then use Dan's custom script to remove unwanted sequences after bowtie alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bowtie output lines read: 27478 5089t lines read: 8637d: 14625 lines read: 17738owtie output lines read: 20232 lines read: 22858umber of Bowtie output lines read: 25545\n",
      "Number of sequences written to output: 27476ritten to output: 2655written to output: 646nces written to output: 789mber of sequences written to output: 956itten to output: 1166 output: 1352 of sequences written to output: 1562mber of sequences written to output: 1725sequences written to output: 1911s written to output: 2097 2306mber of sequences written to output: 2493sequences written to output: 2679s written to output: 2865n to output: 3051itten to output: 3214 output: 3400n to output: 3563mber of sequences written to output: 3773s written to output: 3889mber of sequences written to output: 40291 of sequences written to output: 4378n to output: 4587mber of sequences written to output: 4797mber of sequences written to output: 5053itten to output: 52621 output: 5704 of sequences written to output: 5914 of sequences written to output: 6170n to output: 6379mber of sequences written to output: 6589mber of sequences written to output: 6845itten to output: 7054 output: 7240 output: 7496 of sequences written to output: 7706 of sequences written to output: 7962 8194ences written to output: 8404mber of sequences written to output: 8637itten to output: 8846itten to output: 91021 output: 9544 9730n to output: 9963ritten to output: 10237ten to output: 10510n to output: 10692o output: 10965output: 11147itten to output: 11352mber of sequences written to output: 11580 to output: 11807s written to output: 12012ritten to output: 12285umber of sequences written to output: 12513nces written to output: 12763o output: 130133umber of sequences written to output: 13537nces written to output: 137874014 sequences written to output: 14265tten to output: 14515ber of sequences written to output: 14743 of sequences written to output: 15016written to output: 15266ber of sequences written to output: 15767es written to output: 16017written to output: 16290put: 16540ences written to output: 16768to output: 1701868equences written to output: 17519en to output: 1776918019f sequences written to output: 18270itten to output: 18520mber of sequences written to output: 18748 to output: 18975 output: 19248 sequences written to output: 19385tten to output: 19635ber of sequences written to output: 19863r of sequences written to output: 20045of sequences written to output: 202271tten to output: 20659ences written to output: 20864to output: 21114 written to output: 21319sequences written to output: 21524quences written to output: 21706ber of sequences written to output: 2191122115 output: 22320tput: 22593t: 22866r of sequences written to output: 23117 written to output: 23367uences written to output: 23845er of sequences written to output: 24050 of sequences written to output: 2423236ten to output: 24846n to output: 25028to output: 252103691put: 25756t: 25938 to output: 26143o output: 26325written to output: 26530tput: 26689ut: 26871ber of sequences written to output: 2703127235417\n"
     ]
    }
   ],
   "source": [
    "!python parseBowtie_DD.py batch_100_BOWTIEout.sam batch_100_BOWTIEout_filtered.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I ran this command in the terminal outside jupyter notebook because the auto updating of the sys.stout.write() was freakin' out the notebook.\n",
    "\n",
    "```\n",
    "python parseBowtie_DD.py batch_100_BOWTIEout.sam batch_100_BOWTIEout_filtered.fa\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "Number of Bowtie output lines read: 27478\n",
    "Number of sequences written to output: 27476\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To see how many loci were filtered out due to Bowtie, first count how many unique loci were in the genepop file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your genepop file has 232763 columns of genotypes.\r\n"
     ]
    }
   ],
   "source": [
    "!python count_loci_genepop.py Stacks/batch_100.genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I think that means Bowtie filtered out a ton of loci. 232763-27478 = 205285 = 88%\n",
    "\n",
    "That's sort of insane compared to when I ran through the Pacific cod data, but I did use extremely lenient parameters on this run of populations.\n",
    "\n",
    "## Filtering with Blast\n",
    "\n",
    "Make a blast database with filtered output from Bowtie filtering, then blast sequences to database and remove any loci that match other loci equally well or better than to themselves. This is supposed to remove highly repetitive loci like microsatellites that can interfere with our data analysis.\n",
    "\n",
    "Need to install Blast, make sure I can run it from anywhere, make a directory for Blast files, move the filtered bowtie fasta there, then blast sequences to the database and filter with Dan's Blast parsing script.\n",
    "\n",
    "First step... install Blast on Ubuntu:\n",
    "\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder$ sudo apt-get install ncbi-blast+\n",
    "```\n",
    "\n",
    "Make sure blast is working:\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder$ blastp\n",
    "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\n",
    "Please refer to the BLAST+ user manual.\n",
    "```\n",
    "Now, make a directory for Blast files.\n",
    "\n",
    "<img src=\"bowtie_folder_made.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Building a new DB, current time: 02/09/2017 11:04:48\n",
      "New DB name:   /mnt/hgfs/Data_for_Analysis/WorkingFolder/Blast/batch_100_BOWTIEfiltered\n",
      "New DB title:  batch_100_BOWTIEout_filtered.fa\n",
      "Sequence type: Nucleotide\n",
      "Keep Linkouts: T\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 27476 sequences in 0.571891 seconds.\n"
     ]
    }
   ],
   "source": [
    "!makeblastdb -in batch_100_BOWTIEout_filtered.fa \\\n",
    "-parse_seqids \\\n",
    "-dbtype nucl \\\n",
    "-out batch_100_BOWTIEfiltered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now blast against itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!blastn -query batch_100_BOWTIEout_filtered.fa \\\n",
    "-db batch_100_BOWTIEfiltered \\\n",
    "-out batch_100_BowtieBlastFiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use Dan's python script to filter only wanted loci:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It was crashing the notebook... in the terminal I ran:\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder/Blast$ python checkBlastResults_DD.py \\\n",
    "> batch_100_BowtieBlastFiltered \\\n",
    "> batch_100_BOWTIEout_filtered.fa \\\n",
    "> batch_100_BowtieBlastFiltered_GOOD.fa \\\n",
    "> batch_100_BowtieBlastFiltered_BAD.fa\n",
    "\n",
    "Identifying which loci are 'good' and 'bad' based on BLAST alignments...\n",
    "Writing 'good' and 'bad' loci to their respective files...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, check how many loci were filtered out during the BLAST phase of filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Data_for_Analysis/WorkingFolder/Blast\n"
     ]
    }
   ],
   "source": [
    "cd Data_for_Analysis/WorkingFolder/Blast/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27460\r\n"
     ]
    }
   ],
   "source": [
    "!grep \">\" batch_100_BowtieBlastFiltered_GOOD.fa | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\r\n"
     ]
    }
   ],
   "source": [
    "!grep \">\" batch_100_BowtieBlastFiltered_BAD.fa | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So that makes 27460 - 16 = 27444, or <.1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating final reference genome with Bowtie\n",
    "\n",
    "Lastly, I need to use Bowtie again to build a final Bowtie index using the files cleaned in Blast, and then use Bowtie to align all of my fastq files to the Bowtie index for pstacks. So I manually need to move the output files from Blast to the Bowtie folder. Its these alignments (of each fastq with the final reference genome) that feed into ``pstacks``.\n",
    "\n",
    "Ran this outside the notebook because it crashes the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!bowtie-build batch_100_BowtieBlastFiltered_GOOD.fa batch_100_final_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, need to align each fastq file against final reference genome. I made a Python script to write that bash script. Going to see if it needs updating...\n",
    "\n",
    "Sample bash script line:\n",
    "``bowtie -q -v 3 --norc --sam Blast/batch_100_final_index Stacks/Q351.fq Q351.sam``\n",
    "\n",
    "Output:\n",
    "\n",
    "...\n",
    "```\n",
    "Reported 2500668 alignments to 1 output stream(s)\n",
    "# reads processed: 3153859\n",
    "# reads with at least one reported alignment: 2269973 (71.97%)\n",
    "# reads that failed to align: 883886 (28.03%)\n",
    "Reported 2269973 alignments to 1 output stream(s)\n",
    "\n",
    "Running bowtie took \n",
    "08:10:31.52\n",
    "```\n",
    "\n",
    "Now into ``pstacks``!\n",
    "\n",
    "## ``pstacks``\n",
    "\n",
    "Wrote a ``pstacks`` bash with a script I made last quarter.\n",
    "\n",
    "Sample ``pstacks`` line of code:\n",
    "\n",
    "``stacks pstacks -p 5 -m 10 -t sam -i 1 -f Stacks/Q351.sam -o Stacks``\n",
    "\n",
    "Output:\n",
    "\n",
    "...\n",
    "```\n",
    "Parsing Stacks/FG207.sam\n",
    "  Analyzed 2269973 sequence reads; Identified 146488 unique stacks from those reads.\n",
    "  Merged 149407 unique Stacks into 17980 loci.\n",
    "Identifying polymorphic sites and calling consensus sequences...done.\n",
    "  Number of utilized reads 2269973\n",
    "  Mean coverage depth is 129.1; Std Dev: 243.874; Max: 14096\n",
    "Writing loci, SNPs, alleles to 'Stacks/...'\n",
    "  Wrote 17573 loci, excluded 407 loci due to insuffient depth of coverage; blacklisted 0 loci.\n",
    "Finished running pstacks_shell.txt script.\n",
    "\n",
    "Running pstacks took \n",
    "01:23:37.17\n",
    "\n",
    "```\n",
    "\n",
    "## Second round ``cstacks``\n",
    "\n",
    "I'm going to use my ``easy_cstacks.py`` script. Notice that now we have to tags, snps, and alleles file, one set from ustacks and one now from pstacks. I moved all the pstacks ones into their own directory (called pstacks).\n",
    "\n",
    "This is what the bash script looks like:\n",
    "\n",
    "```\n",
    "Currently, this program thinks you intend to include 47 samples in cstacks.\n",
    "\n",
    "Type YES if correct. Type NO if incorrect and check your files and code.YES\n",
    "\n",
    "Sample number verified. Program continuing.\n",
    "\n",
    "Finished writing cstacks shell script.\n",
    "\n",
    "This is what your cstacks shell looks like.\n",
    "stacks cstacks -b 100 -g -s pstacks/Q324 -s pstacks/Q318 -s pstacks/Q326 -s pstacks/Q314 -s pstacks/Q316 -s pstacks/Q323 -s pstacks/Q325 -s pstacks/Q351 -s pstacks/Q352 -s pstacks/Q332 -s pstacks/Q339 -s pstacks/Q334 -s pstacks/Q347 -s pstacks/Q356 -s pstacks/Q330 -s pstacks/Q355 -s pstacks/Q354 -s pstacks/FG009 -s pstacks/FG033 -s pstacks/FG002 -s pstacks/FG027 -s pstacks/FG021 -s pstacks/FG013 -s pstacks/FG001 -s pstacks/FG005 -s pstacks/FG014 -s pstacks/FG006 -s pstacks/FG101_A -s pstacks/FG102_A -s pstacks/FG100_B -s pstacks/FG106 -s pstacks/FG107 -s pstacks/FG109 -s pstacks/FG104_A -s pstacks/FG111 -s pstacks/FG108 -s pstacks/FG103_A -s pstacks/FG210 -s pstacks/FG205 -s pstacks/FG211 -s pstacks/FG212 -s pstacks/FG201 -s pstacks/FG209 -s pstacks/FG206 -s pstacks/FG204 -s pstacks/FG202 -s pstacks/FG203 -o pstacks -n 3 -p 5 \n",
    "Run this shell script?YES\n",
    "Running shell script.\n",
    "Number of mismatches allowed between stacks: 3\n",
    "Loci matched based on genomic location.\n",
    "Constructing catalog from 47 samples.\n",
    "Initializing new catalog...\n",
    "  Parsing pstacks/Q324.tags.tsv\n",
    "  Parsing pstacks/Q324.snps.tsv\n",
    "  Parsing pstacks/Q324.alleles.tsv\n",
    "Building an index of the catalog.\n",
    "Processing sample pstacks/Q324 [2 of 47]\n",
    "  Parsing pstacks/Q318.tags.tsv\n",
    "  Parsing pstacks/Q318.snps.tsv\n",
    "  Parsing pstacks/Q318.alleles.tsv\n",
    "Searching for matches by genomic location...\n",
    "Merging matches into catalog...\n",
    "  0 loci matched more than one catalog locus and were excluded.\n",
    "  Updating catalog index...\n",
    "Processing sample pstacks/Q318 [3 of 47]\n",
    "  Parsing pstacks/Q326.tags.tsv\n",
    "\n",
    "```\n",
    "\n",
    "The output looks like:\n",
    "\n",
    "...\n",
    "```\n",
    "Processing sample pstacks/FG204 [46 of 47]\n",
    "  Parsing pstacks/FG202.tags.tsv\n",
    "  Parsing pstacks/FG202.snps.tsv\n",
    "  Parsing pstacks/FG202.alleles.tsv\n",
    "Searching for matches by genomic location...\n",
    "Merging matches into catalog...\n",
    "  0 loci matched more than one catalog locus and were excluded.\n",
    "  Updating catalog index...\n",
    "Processing sample pstacks/FG202 [47 of 47]\n",
    "  Parsing pstacks/FG203.tags.tsv\n",
    "  Parsing pstacks/FG203.snps.tsv\n",
    "  Parsing pstacks/FG203.alleles.tsv\n",
    "Searching for matches by genomic location...\n",
    "Merging matches into catalog...\n",
    "  0 loci matched more than one catalog locus and were excluded.\n",
    "  Updating catalog index...\n",
    "Writing catalog to 'pstacks/... done.\n",
    "\n",
    "Running cstacks took \n",
    "00:15:31.07\n",
    "\n",
    "```\n",
    "It ran so fast though - going to double-check that nothing is fishy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_100.catalog.tags.tsv:27460\r\n"
     ]
    }
   ],
   "source": [
    "!grep --count --with-filename consensus batch_100.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looks like I have 27460 loci in my catalog, that looks about normal (Eleni had 29K loci in her catalog).\n",
    "\n",
    "## Second ``sstacks``\n",
    "\n",
    "First line of the bash script looked like:\n",
    "\n",
    "```\n",
    "stacks sstacks -g -b 100 -s pstacks/Q351 -o pstacks -p 3 -c pstacks/batch_100\n",
    "```\n",
    "\n",
    "Output looked like:\n",
    "\n",
    "...\n",
    "```\n",
    "Processing sample 'pstacks/FG207' [1 of 1]\n",
    "  Parsing pstacks/FG207.tags.tsv\n",
    "  Parsing pstacks/FG207.snps.tsv\n",
    "  Parsing pstacks/FG207.alleles.tsv\n",
    "Searching for matches by genomic location...\n",
    "  Creating map of genomic locations...done.\n",
    "17573 stacks matched against the catalog containing 27460 loci.\n",
    "  17573 matching loci, 1527 contained no verified haplotypes.\n",
    "  1204 loci contained SNPs unaccounted for in the catalog and were excluded.\n",
    "  23178 total haplotypes examined from matching loci, 19817 verified.\n",
    "Outputing to file pstacks/FG207.matches.tsv\n",
    "00:24:00.03\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Second ``populations``\n",
    "\n",
    "``\n",
    "stacks populations -b 100 -P Stacks -M popmap_cragigrun1.txt -t 10 -r 1 -p 5 -m 5 --genepop\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Data_for_Analysis/WorkingFolder'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fst kernel smoothing: off\n",
      "Bootstrap resampling: off\n",
      "Percent samples limit per population: 1\n",
      "Locus Population limit: 5\n",
      "Minimum stack depth: 10\n",
      "Log liklihood filtering: off; threshold: 0\n",
      "Minor allele frequency cutoff: 0\n",
      "Maximum observed heterozygosity cutoff: 1\n",
      "Applying Fst correction: none.\n",
      "Parsing population map.\n",
      "Found 71 input file(s).\n",
      "  5 populations found\n",
      "    WA_Strait: FG001, FG002, FG003, FG004, FG005, FG006, FG007, FG009, FG010, FG011, FG013, FG014, FG015, FG016, FG017, FG018, FG019, FG021, FG024, FG026, FG027, FG031, FG032, FG033, FG034, FG035\n",
      "    WA_SanJuans: FG100_A, FG100_B, FG101_A, FG101_B, FG102_A, FG102_B, FG103_A, FG103_B, FG104_A, FG104_B, FG106, FG107, FG108, FG109, FG111\n",
      "    WA_Dabob: FG201, FG202, FG203, FG204, FG205, FG206, FG207, FG209, FG210, FG211, FG212\n",
      "    AK: Q314, Q316, Q318, Q323, Q324, Q325, Q326\n",
      "    CA_Catalina: Q330, Q332, Q334, Q338, Q339, Q347, Q351, Q352, Q353, Q354, Q355, Q356\n",
      "  1 group of populations found\n",
      "    1: WA_Strait, WA_SanJuans, WA_Dabob, AK, CA_Catalina\n",
      "  Parsing pstacks/batch_100.catalog.tags.tsv\n",
      "  Parsing pstacks/batch_100.catalog.snps.tsv\n",
      "  Parsing pstacks/batch_100.catalog.alleles.tsv\n",
      "  Parsing pstacks/FG001.matches.tsv\n",
      "  Parsing pstacks/FG002.matches.tsv\n",
      "  Parsing pstacks/FG003.matches.tsv\n",
      "  Parsing pstacks/FG004.matches.tsv\n",
      "  Parsing pstacks/FG005.matches.tsv\n",
      "  Parsing pstacks/FG006.matches.tsv\n",
      "  Parsing pstacks/FG007.matches.tsv\n",
      "  Parsing pstacks/FG009.matches.tsv\n",
      "  Parsing pstacks/FG010.matches.tsv\n",
      "  Parsing pstacks/FG011.matches.tsv\n",
      "  Parsing pstacks/FG013.matches.tsv\n",
      "  Parsing pstacks/FG014.matches.tsv\n",
      "  Parsing pstacks/FG015.matches.tsv\n",
      "  Parsing pstacks/FG016.matches.tsv\n",
      "  Parsing pstacks/FG017.matches.tsv\n",
      "  Parsing pstacks/FG018.matches.tsv\n",
      "  Parsing pstacks/FG019.matches.tsv\n",
      "  Parsing pstacks/FG021.matches.tsv\n",
      "  Parsing pstacks/FG024.matches.tsv\n",
      "  Parsing pstacks/FG026.matches.tsv\n",
      "  Parsing pstacks/FG027.matches.tsv\n",
      "  Parsing pstacks/FG031.matches.tsv\n",
      "  Parsing pstacks/FG032.matches.tsv\n",
      "  Parsing pstacks/FG033.matches.tsv\n",
      "  Parsing pstacks/FG034.matches.tsv\n",
      "  Parsing pstacks/FG035.matches.tsv\n",
      "  Parsing pstacks/FG100_A.matches.tsv\n",
      "Warning: unable to find any matches in file 'FG100_A', excluding this sample from population analysis.\n",
      "  Parsing pstacks/FG100_B.matches.tsv\n",
      "  Parsing pstacks/FG101_A.matches.tsv\n",
      "  Parsing pstacks/FG101_B.matches.tsv\n",
      "  Parsing pstacks/FG102_A.matches.tsv\n",
      "  Parsing pstacks/FG102_B.matches.tsv\n",
      "  Parsing pstacks/FG103_A.matches.tsv\n",
      "  Parsing pstacks/FG103_B.matches.tsv\n",
      "  Parsing pstacks/FG104_A.matches.tsv\n",
      "  Parsing pstacks/FG104_B.matches.tsv\n",
      "  Parsing pstacks/FG106.matches.tsv\n",
      "  Parsing pstacks/FG107.matches.tsv\n",
      "  Parsing pstacks/FG108.matches.tsv\n",
      "  Parsing pstacks/FG109.matches.tsv\n",
      "  Parsing pstacks/FG111.matches.tsv\n",
      "  Parsing pstacks/FG201.matches.tsv\n",
      "  Parsing pstacks/FG202.matches.tsv\n",
      "  Parsing pstacks/FG203.matches.tsv\n",
      "  Parsing pstacks/FG204.matches.tsv\n",
      "  Parsing pstacks/FG205.matches.tsv\n",
      "  Parsing pstacks/FG206.matches.tsv\n",
      "  Parsing pstacks/FG207.matches.tsv\n",
      "  Parsing pstacks/FG209.matches.tsv\n",
      "  Parsing pstacks/FG210.matches.tsv\n",
      "  Parsing pstacks/FG211.matches.tsv\n",
      "  Parsing pstacks/FG212.matches.tsv\n",
      "  Parsing pstacks/Q314.matches.tsv\n",
      "  Parsing pstacks/Q316.matches.tsv\n",
      "  Parsing pstacks/Q318.matches.tsv\n",
      "  Parsing pstacks/Q323.matches.tsv\n",
      "  Parsing pstacks/Q324.matches.tsv\n",
      "  Parsing pstacks/Q325.matches.tsv\n",
      "  Parsing pstacks/Q326.matches.tsv\n",
      "  Parsing pstacks/Q330.matches.tsv\n",
      "  Parsing pstacks/Q332.matches.tsv\n",
      "  Parsing pstacks/Q334.matches.tsv\n",
      "  Parsing pstacks/Q338.matches.tsv\n",
      "  Parsing pstacks/Q339.matches.tsv\n",
      "  Parsing pstacks/Q347.matches.tsv\n",
      "  Parsing pstacks/Q351.matches.tsv\n",
      "  Parsing pstacks/Q352.matches.tsv\n",
      "  Parsing pstacks/Q353.matches.tsv\n",
      "  Parsing pstacks/Q354.matches.tsv\n",
      "  Parsing pstacks/Q355.matches.tsv\n",
      "  Parsing pstacks/Q356.matches.tsv\n",
      "Populating observed haplotypes for 70 samples, 27460 loci.\n",
      "Removed 90 samples from loci that are below the minimum stack depth of 10x\n",
      "Removing 26609 loci that did not pass sample/population constraints... retained 851 loci.\n",
      "Loading model outputs for 70 samples, 851 loci.\n",
      "  Parsing pstacks/FG001.tags.tsv\n",
      "  Parsing pstacks/FG002.tags.tsv\n",
      "  Parsing pstacks/FG003.tags.tsv\n",
      "  Parsing pstacks/FG004.tags.tsv\n",
      "  Parsing pstacks/FG005.tags.tsv\n",
      "  Parsing pstacks/FG006.tags.tsv\n",
      "  Parsing pstacks/FG007.tags.tsv\n",
      "  Parsing pstacks/FG009.tags.tsv\n",
      "  Parsing pstacks/FG010.tags.tsv\n",
      "  Parsing pstacks/FG011.tags.tsv\n",
      "  Parsing pstacks/FG013.tags.tsv\n",
      "  Parsing pstacks/FG014.tags.tsv\n",
      "  Parsing pstacks/FG015.tags.tsv\n",
      "  Parsing pstacks/FG016.tags.tsv\n",
      "  Parsing pstacks/FG017.tags.tsv\n",
      "  Parsing pstacks/FG018.tags.tsv\n",
      "  Parsing pstacks/FG019.tags.tsv\n",
      "  Parsing pstacks/FG021.tags.tsv\n",
      "  Parsing pstacks/FG024.tags.tsv\n",
      "  Parsing pstacks/FG026.tags.tsv\n",
      "  Parsing pstacks/FG027.tags.tsv\n",
      "  Parsing pstacks/FG031.tags.tsv\n",
      "  Parsing pstacks/FG032.tags.tsv\n",
      "  Parsing pstacks/FG033.tags.tsv\n",
      "  Parsing pstacks/FG034.tags.tsv\n",
      "  Parsing pstacks/FG035.tags.tsv\n",
      "  Parsing pstacks/FG100_B.tags.tsv\n",
      "  Parsing pstacks/FG101_A.tags.tsv\n",
      "  Parsing pstacks/FG101_B.tags.tsv\n",
      "  Parsing pstacks/FG102_A.tags.tsv\n",
      "  Parsing pstacks/FG102_B.tags.tsv\n",
      "  Parsing pstacks/FG103_A.tags.tsv\n",
      "  Parsing pstacks/FG103_B.tags.tsv\n",
      "  Parsing pstacks/FG104_A.tags.tsv\n",
      "  Parsing pstacks/FG104_B.tags.tsv\n",
      "  Parsing pstacks/FG106.tags.tsv\n",
      "  Parsing pstacks/FG107.tags.tsv\n",
      "  Parsing pstacks/FG108.tags.tsv\n",
      "  Parsing pstacks/FG109.tags.tsv\n",
      "  Parsing pstacks/FG111.tags.tsv\n",
      "  Parsing pstacks/FG201.tags.tsv\n",
      "  Parsing pstacks/FG202.tags.tsv\n",
      "  Parsing pstacks/FG203.tags.tsv\n",
      "  Parsing pstacks/FG204.tags.tsv\n",
      "  Parsing pstacks/FG205.tags.tsv\n",
      "  Parsing pstacks/FG206.tags.tsv\n",
      "  Parsing pstacks/FG207.tags.tsv\n",
      "  Parsing pstacks/FG209.tags.tsv\n",
      "  Parsing pstacks/FG210.tags.tsv\n",
      "  Parsing pstacks/FG211.tags.tsv\n",
      "  Parsing pstacks/FG212.tags.tsv\n",
      "  Parsing pstacks/Q314.tags.tsv\n",
      "  Parsing pstacks/Q316.tags.tsv\n",
      "  Parsing pstacks/Q318.tags.tsv\n",
      "  Parsing pstacks/Q323.tags.tsv\n",
      "  Parsing pstacks/Q324.tags.tsv\n",
      "  Parsing pstacks/Q325.tags.tsv\n",
      "  Parsing pstacks/Q326.tags.tsv\n",
      "  Parsing pstacks/Q330.tags.tsv\n",
      "  Parsing pstacks/Q332.tags.tsv\n",
      "  Parsing pstacks/Q334.tags.tsv\n",
      "  Parsing pstacks/Q338.tags.tsv\n",
      "  Parsing pstacks/Q339.tags.tsv\n",
      "  Parsing pstacks/Q347.tags.tsv\n",
      "  Parsing pstacks/Q351.tags.tsv\n",
      "  Parsing pstacks/Q352.tags.tsv\n",
      "  Parsing pstacks/Q353.tags.tsv\n",
      "  Parsing pstacks/Q354.tags.tsv\n",
      "  Parsing pstacks/Q355.tags.tsv\n",
      "  Parsing pstacks/Q356.tags.tsv\n",
      "Generating nucleotide-level summary statistics for population 'WA_Strait'\n",
      "Population 'WA_Strait' contained 54 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'WA_SanJuans'\n",
      "Population 'WA_SanJuans' contained 36 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'WA_Dabob'\n",
      "Population 'WA_Dabob' contained 33 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'AK'\n",
      "Population 'AK' contained 21 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'CA_Catalina'\n",
      "Population 'CA_Catalina' contained 34 incompatible loci -- more than two alleles present.\n",
      "Tallying loci across populations...done.\n",
      "Pruned 277 variant sites due to filter constraints.\n",
      "  (enable the --verbose flag to record the reason why each site was filtered in the batch_X.populations.log file.)\n",
      "Removing 4 additional loci for which all variant sites were filtered... retained 847 loci.\n",
      "Regenerating nucleotide-level summary statistics for population 'WA_Strait'\n",
      "Population 'WA_Strait' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'WA_SanJuans'\n",
      "Population 'WA_SanJuans' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'WA_Dabob'\n",
      "Population 'WA_Dabob' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'AK'\n",
      "Population 'AK' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'CA_Catalina'\n",
      "Population 'CA_Catalina' contained 0 incompatible loci -- more than two alleles present.\n",
      "Re-tallying loci across populations...done.\n",
      "Generating haplotype-level summary statistics for population 'WA_Strait'\n",
      "Generating haplotype-level summary statistics for population 'WA_SanJuans'\n",
      "Generating haplotype-level summary statistics for population 'WA_Dabob'\n",
      "Generating haplotype-level summary statistics for population 'AK'\n",
      "Generating haplotype-level summary statistics for population 'CA_Catalina'\n",
      "Writing 847 loci to summary statistics file, 'pstacks/batch_100.sumstats.tsv'\n",
      "Writing 847 loci to observed haplotype file, 'pstacks/batch_100.haplotypes.tsv'\n",
      "Writing population data to GenePop file 'pstacks/batch_100.genepop'\n"
     ]
    }
   ],
   "source": [
    "!stacks populations -b 100 -P pstacks -M popmap_cragigrun1.txt -t 10 -r 1 -p 5 -m 10 --genepop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your genepop file has 5027 columns of genotypes.\r\n"
     ]
    }
   ],
   "source": [
    "!python count_loci_genepop.py pstacks/batch_100.genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Filtering shenanigans\n",
    "\n",
    "Everyone in our lab uses a whole bunch of filtering scripts that I haven't used before, so I'll be trying to make sense of them here.\n",
    "\n",
    "Looking at Mary's jupyter [notebook](https://github.com/mfisher5/mf-fish546-PCod/blob/master/notebooks/Lanes%201%20and%202%20combined%20pipeline.ipynb) on this topic for guidance, and Eleni's Evernote page about filtering.\n",
    "\n",
    "#### [1] Remove heading from catalog SNPs file\n",
    "\n",
    "I think Mary does this manually. Going to look at the file and see if I can sort out why... looks like it's just that commented out line. I wrote a short script to remove the first n lines, and removed the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cstacks version 1.35; catalog generated on 2017-02-14 10:51:19\r\n",
      "0\t100\t1\t42\tE\t0\tT\tA\t-\t-\r\n",
      "0\t100\t1\t126\tE\t0\tT\tA\t-\t-\r\n",
      "0\t100\t1\t127\tE\t0\tT\tA\t-\t-\r\n",
      "0\t100\t1\t128\tE\t0\tA\tT\t-\t-\r\n",
      "0\t100\t1\t129\tE\t0\tT\tA\t-\t-\r\n",
      "0\t100\t1\t130\tE\t0\tT\tA\t-\t-\r\n",
      "0\t100\t2\t57\tE\t0\tC\tT\t-\t-\r\n",
      "0\t100\t2\t127\tE\t0\tT\tC\t-\t-\r\n",
      "0\t100\t3\t10\tE\t0\tT\tC\t-\t-\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Your text file has 230573 lines, and you want to extract lines 1 and write it to the file with name ../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100_woheader.catalog.snps.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!python remove_first_n_lines_textfile.py ../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100.catalog.snps.tsv 1 ../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100_woheader.catalog.snps.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Data_for_Analysis/WorkingFolder/pstacks\n"
     ]
    }
   ],
   "source": [
    "cd Data_for_Analysis/WorkingFolder/pstacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t100\t1\t42\tE\t0\tT\tA\t-\t-\r\n",
      "\r\n",
      "0\t100\t1\t126\tE\t0\tT\tA\t-\t-\r\n",
      "\r\n",
      "0\t100\t1\t127\tE\t0\tT\tA\t-\t-\r\n",
      "\r\n",
      "0\t100\t1\t128\tE\t0\tA\tT\t-\t-\r\n",
      "\r\n",
      "0\t100\t1\t129\tE\t0\tT\tA\t-\t-\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head batch_100_woheader.catalog.snps.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] add a \"_\" between Cat and ID in the batch_1 haplotypes.tsv file\n",
    "\n",
    "I think Mary did this manually again. So first unzip, then manually open in a text editor, and add an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: batch_100.haplotypes.tsv: unknown suffix -- ignored\r\n"
     ]
    }
   ],
   "source": [
    "!gzip -d batch_100.haplotypes.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check that the haplotypes file looks good. It does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog_ID\tCnt\tFG001\tFG002\tFG003\tFG004\tFG005\tFG006\tFG007\tFG009\tFG010\tFG011\tFG013\tFG014\tFG015\tFG016\tFG017\tFG018\tFG019\tFG021\tFG024\tFG026\tFG027\tFG031\tFG032\tFG033\tFG034\tFG035\tFG100_B\tFG101_A\tFG101_B\tFG102_A\tFG102_B\tFG103_A\tFG103_B\tFG104_A\tFG104_B\tFG106\tFG107\tFG108\tFG109\tFG111\tFG201\tFG202\tFG203\tFG204\tFG205\tFG206\tFG207\tFG209\tFG210\tFG211\tFG212\tQ314\tQ316\tQ318\tQ323\tQ324\tQ325\tQ326\tQ330\tQ332\tQ334\tQ338\tQ339\tQ347\tQ351\tQ352\tQ353\tQ354\tQ355\tQ356\r\n",
      "119\t70\tCGTACAGT/CGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT/TGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGG/CGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCATATAGT/CGTATAGG\tCGTATAGT\tCGTATAGT/TGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATACT/CGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGCATTGT/CGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTACAGT/CGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTACAGT/CGTATAGT\tCGTATAGT/CGTGTAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\r\n",
      "182\t70\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/TCACTACGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGAATCCGGCGAGC/CCATTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGTTC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGTTC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCCGGGGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGAATCCGGCGAGC/CCACTCCGGGGCCGGCGAGA\tCCACTCAGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/TCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGAATCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGAATCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGTTC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGTCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGTCGGCGAGC\tCCACGCCGGGGCCGGCGAGC/CCGCTCCGGGGCCCGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACGCCGGGGCCGGCGAGC/CCGCTCCGGGGCCCGCGAGC\tCCACTCCGGGGCCGGCGAGC/TCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/TCACTCCGGGGCCGGCGAGC\tCCACTCAGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCAGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCAACTCCGGGGCCGGCGATC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCGCTCCGGGGCCCGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/TCACTACGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCGGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCGGGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGAATCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGA/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCAGGGCCGGCGAGC/CCACTCCGAATCCGGCGAGC\tCCACTCCGGGGCAGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACTACGGGGCCGGCGAGC/TCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCAAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGTTC/CCGCTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGACGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCAGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\r\n",
      "190\t70\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG/GTG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tACG/ATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG/GTG\tATG\tATA/ATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATA/ATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tACG/ATG\tATG\tATG\tATG\tATG\tATG\tATG\r\n",
      "196\t70\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG/T\tG/T\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG/T\tG\tG\tG\tG\tG\tG\tG/T\tG\tG\tG\tG\r\n",
      "251\t70\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA/ACCC\tACCA\tACCA\tACCA\tACCA\tACCA/GCCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA/ACGA\tACCA/GCCA\tACCA\tACCA/GCCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA/ATCA\tACCA/ATCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA/ATCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA/ATCA\tACCA\tACCA\tACCA\tACCA\tACCA/GCCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\r\n",
      "253\t70\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC/TTC\tGTC\tGTC\tGTC/TTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC/TTC\tGTC/TTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC/GTT\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC/TTC\tGTC\tGTC\tGTC\tGTC\tGAC/GTC\tGTC\tGTC\tGTC/TTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\r\n",
      "256\t70\tTGG\tCGG/TGG\tCGG/TGG\tTGG\tCGG/TGG\tCGG/TGG\tCGG\tCGG\tCGG/TGG\tCGG\tTGG\tCGA/CGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tTGG\tCGG/TGG\tTGG\tCGG/TGG\tTGG\tCGG\tCGG/TGG\tCGG\tCGG/TGG\tTGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG\tCGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG\tCGG/TGG\tCGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG\tCGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tTGG\tTGG\tCTG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG\tTGG\tCGG\tCGG/TGG\tCGG\tCGG/TGG\r\n",
      "303\t70\tGACG\tAACG/GACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tAACG\tGACG\tGACG\tAACG/GACG\tGACG/GATG\tGACG/GATG\tGACG\tAACG/GACG\tGACA/GACG\tGACG\tGACG\tAACG/GACG\tGACG\tGACG\tAACG/GACG\tAACG/GACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tAACG/GACG\tGACG\tGACG\tGACA\tGACG\tGACG\tGACG/GGCG\tGACA\tGACG\tGACG/GATG\tAACG/GACG\tAACG/GACG\tGACG\tGACG\tGACG\tGACG\tGACG\tAACG/GACG\tGACG\tGACG\tGACG\tGACG\tGACA/GACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACA/GACG\tAACG\r\n",
      "352\t70\tAGGACAAA/ATGACAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA/AGGGCAAA/TGGAGAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA\tAGGACAAA\tAGGACCAA\tAGGACAAA\tAGGGCAAA/ATGACAAA\tAGGACAAA\tAGGACAAA/TTGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA\tAGGACAAA\tATGACAAA/TTGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA\tATGACAAA\tAGGACAAA/AGGGCAAA\tAGGGCAAA/ATGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA\tAGGACAAA/AGGGCAAC\tAGGGCAAA/TTGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA\tAGGACAAA\tAGGGCAAA/ATGACAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tAGAACAAA/AGGACAAA/AGGGCAAA\tAGGACAAA\tAGGGCAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/TTGACAAA\tAGGGCAAA/ATGACAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tTGGAGAAA/TTGACAAA\tAGGGCAAA/ATGACAGA\tAGGACAAA\tAGGACAAA\tATGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA\tAGGGCAAA/ATGACAAC\r\n"
     ]
    }
   ],
   "source": [
    "!head batch_100.haplotypes.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3] Run Marine's script to make biallelic catalog reference.\n",
    "\n",
    "Here's a [link to the script](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Scripts/preparing_file_for_correcting_genotypes.py.txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Ubuntu_Bioinformatics/Git_repo/CRAGIG_run1/Scripts\n"
     ]
    }
   ],
   "source": [
    "cd /mnt/hgfs/Ubuntu_Bioinformatics/Git_repo/CRAGIG_run1/Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 70 CGTACAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT TGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGG CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CATATAGT CGTATAGG CGTATAGT CGTATAGT TGTATAGT CGTATAGT CGTATAGT CGTATACT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGCATTGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTACAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTACAGT CGTATAGT CGTATAGT CGTGTAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT\r\n",
      "182 70 CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC TCACTACGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCATTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGTTC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGTTC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGGGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCACTCCGGGGCCGGCGAGA CCACTCAGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC TCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGTTC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGTCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGTCGGCGAGC CCACGCCGGGGCCGGCGAGC CCGCTCCGGGGCCCGCGAGC CCGCTCCGGGGCCGGCGAGC CCACGCCGGGGCCGGCGAGC CCGCTCCGGGGCCCGCGAGC CCACTCCGGGGCCGGCGAGC TCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC TCACTCCGGGGCCGGCGAGC CCACTCAGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCAGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CAACTCCGGGGCCGGCGATC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCGCTCCGGGGCCCGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC TCACTACGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCGCTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCGGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCGCTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCGGGCGAGC CCGCTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGA CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCAGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCACTCCGGGGCAGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCGCTCCGGGGCCGGCGAGC CCACTACGGGGCCGGCGAGC TCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCAAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGTTC CCGCTCCGGGGCCGGCGAGC CCACTCCGGGGCCGACGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCAGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCGCTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC\r\n",
      "190 70 ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG GTG ATG ATG ATG ATG ATG ATG ATG ACG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG GTG ATG ATA ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATA ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ACG ATG ATG ATG ATG ATG ATG ATG\r\n",
      "196 70 G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G T G T G G G G G G G G G G G G G G G G G G G G G G G G G G T G G G G G G G T G G G G\r\n",
      "251 70 ACCA ACCA ACCA ACCA ACCA ACCA ACCC ACCA ACCA ACCA ACCA ACCA GCCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACGA ACCA GCCA ACCA ACCA GCCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ATCA ACCA ATCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ATCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ATCA ACCA ACCA ACCA ACCA ACCA GCCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA\r\n",
      "253 70 GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC TTC GTC GTC GTC TTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC TTC GTC TTC GTC GTC GTC GTC GTC GTC GTC GTC GTT GTC GTC GTC GTC GTC GTC GTC TTC GTC GTC GTC GTC GAC GTC GTC GTC GTC TTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC\r\n",
      "256 70 TGG CGG TGG CGG TGG TGG CGG TGG CGG TGG CGG CGG CGG TGG CGG TGG CGA CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG TGG CGG TGG TGG CGG TGG TGG CGG CGG TGG CGG CGG TGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG CGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG CGG TGG CGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG CGG CGG TGG CGG TGG CGG TGG TGG TGG CTG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG CGG TGG CGG CGG TGG\r\n",
      "303 70 GACG AACG GACG GACG GACG GACG GACG GACG GACG GACG GACG GACG GACG GACG AACG GACG GACG AACG GACG GACG GATG GACG GATG GACG AACG GACG GACA GACG GACG GACG AACG GACG GACG GACG AACG GACG AACG GACG GACG GACG GACG GACG GACG GACG GACG AACG GACG GACG GACG GACA GACG GACG GACG GGCG GACA GACG GACG GATG AACG GACG AACG GACG GACG GACG GACG GACG GACG AACG GACG GACG GACG GACG GACG GACA GACG GACG GACG GACG GACG GACG GACG GACG GACG GACG GACA GACG AACG\r\n",
      "352 70 AGGACAAA ATGACAAA AGGACAAA AGGACAAA AGGACAAA AGGGCAAA TGGAGAAA AGGACAAA AGGACAAA ATGACAAA AGGACAAA ATGACAAA AGGACAAA ATGACAAA AGGACAAA AGGACAAA AGGACCAA AGGACAAA AGGGCAAA ATGACAAA AGGACAAA AGGACAAA TTGACAAA AGGACAAA AGGGCAAA AGGACAAA AGGACAAA ATGACAAA TTGACAAA AGGACAAA AGGGCAAA AGGACAAA ATGACAAA AGGACAAA ATGACAAA AGGACAAA AGGGCAAA AGGACAAA ATGACAAA AGGACAAA AGGGCAAA AGGGCAAA ATGACAAA AGGACAAA AGGGCAAA AGGACAAA AGGGCAAA AGGACAAA AGGACAAA AGGACAAA ATGACAAA AGGACAAA ATGACAAA AGGACAAA AGGGCAAA AGGACAAA AGGGCAAA AGGACAAA AGGACAAA AGGACAAA AGGACAAA AGGGCAAA AGGACAAA AGGACAAA ATGACAAA AGGACAAA AGGACAAA AGGGCAAC AGGGCAAA TTGACAAA AGGACAAA ATGACAAA AGGACAAA AGGACAAA AGGGCAAA ATGACAAA AGGACAAA AGGACAAA AGGACAAA ATGACAAA AGAACAAA AGGACAAA AGGGCAAA AGGACAAA AGGGCAAA AGGACAAA AGGACAAA ATGACAAA AGGACAAA TTGACAAA AGGGCAAA ATGACAAA AGGACAAA AGGACAAA AGGACAAA AGGACAAA ATGACAAA TGGAGAAA TTGACAAA AGGGCAAA ATGACAGA AGGACAAA AGGACAAA ATGACAAA AGGACAAA ATGACAAA AGGACAAA AGGGCAAA ATGACAAC\r\n",
      "357 70 CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAATA CACAGTA CACAGTA CACAGCG CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA TACAGTA CACAGTA CACAGTA CACAGTA CACAGCG CACAGTA CACAGTG CACAGCA CACAGCG CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CAGAGTA CACAGTA CACAGTA CGCAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACTGTA CACAGTA TACAGTA CACAGTA CACAGTA CACAGTA CACAGCA CACAGCG CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGCG CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA\r\n",
      "401 70 GTATT GGATT GTATT GTATT GTAAT GTATT GTATT GGATT GTATT GGATT GTATT GTTTT GGATT GTATT GTATT GTATT GGATT GTATT GTATT GTATT GTATA GTATT GTATT GTATT GTATT GGATT GTATT GTATT GTATT GTAAT GTATT GTATT GGATT GTATT GGATT GTATT GTATT GGATT GGATT GTATT GTATT GTATT GTATT GGATT GTATT GGATT GTATT CTATT GTATT GTATT GTATT GGATT GTATT GGATT GTATT GGATT GTATT GTATT GTATT GTATT GTATT GGATT GTATT GTATT GTATT GTATT GGATT GTATT GTATT GGATT GTATT GTATT GGATT GTATT GGATT GTATT GGATT GTAAT GTATT GGATT GTATA GTATT GTATT GGATT GTAAT GGATT GTATT GGATT GTATT GTATT GTATT GTATT GTATT GGATT GTATT\r\n",
      "471 70 GACAA GACAA GACAA GACAA GACCA GACAA GACCA GACAA GACAA GACAA GACAA GACAA GACAA TACAA GACAA GACAA GACAA GACAA GACCA GACAA GACCA GACAA GACAA GACCA GACCA GACCA GACAA GACAA GACAA GACAA GACCA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACCA GACAA GACAA GACAA GACAC GACAA GACCA GACAA GACAA GACAA GACAA GACCA GACAA GACAA GACAA GACCA GACAA GACAA GACAA GACAA GACAA GATAA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACAC GACAA GACCA GACAA GACCA GACAA GACAA GACAA GACAA GACAA GACAA GACCA GTCAA\r\n",
      "476 70 GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA AA GC GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA\r\n",
      "555 70 GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT TTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GGG GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT\r\n",
      "572 70 CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATAACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATAACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCGC CGGGTATGACCAC CGGGTATGAGCAC CGGGTACGACCAC CGGGTATGACCAC CGGCTATGACCAC CGGGTTTGACCAC CGGGTATGACCAC CGGGTATGACCAT CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTTTGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGAGCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAT CGGGTACGACCAC CTGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATAACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGTGTATGATCAC CGGGTATGACCAC CGTGTATGATCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATAACCAC CGGGTATGACCAC CGGGTATGACCAC TGGGTATGACCAC CGGGTATGGCCAC CGGGTATGACCAC CGGGTTTGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTTTGACCAC CGGGTATGACCAC CGGGTATGACCAT CGGGTATGACCAC CGGGGATGACCAT CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACGAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATAACCAC CGGGTTTGACCAC CGGCTATGACCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTTTGACCAC CTGGTATGACCAC CGGGTATGACCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGAGCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC\r\n",
      "588 70 GGGTGCTCCCTACTG GTGTTCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGGTGCTCCCTACTG GGCTGCTCCATACTG GGGTGTTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTA GGGTGCTCCCTACTG GGGTGCTCCCTACAG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTTCCTAATG AGGTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCATACTG GGGTGCTCTCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGTTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCCCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTTCTG GGGTGCTCCCTACTG GGGTGCTCCCTACAG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCGACTG GGCTGCTCCCGACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCATACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCATACTG GGGTGCTCCCTACTG GGGTGCTCCCTACAG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGTTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGTTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCATACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGGTGCTTCCTAATG GGGTGCTTCCTACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTACTCCCTACTG GGGTGCTCCCTACTG\r\n",
      "625 70 AG TT TT AG TT TT AG TT TT TT TT AG TT TT TT TT TT TT AG TT AG TT AG TT TT AG TG TT TT AG TT TT TT AG TT TT TT TT TT TT TT TT TT AG TT AG TT TT AG TT TT AG TT TT TT AG TT TT AG TT AG TT AG TT AG TG TT TT AG TT TT TT AG AG TT AG TT TT AG TT AG AG AG TT TT AG TT TT TT TT AG TT TT AG TT\r\n",
      "654 70 TAGAG TAGAG TAGAA TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGTG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG CAGAG TAGAG TAGAG TAGAG TAGAG TTGAG TAGAG TAGAG TAAAG TAGAG TAGAG TAGAG TAGAA TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG\r\n",
      "659 70 AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AACC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATT AACC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AACC AATC AATC AATC ATTC AATC AATT AACC AATC AATC AACC AATC AATC AATC AATC AATC AATC AATC ATTC AATC AATC AATC AATC AATC AATC AATC GATC AATC AATC AATC AATC AATC AATC AACC AATC AATC AATC\r\n"
     ]
    }
   ],
   "source": [
    "!python preparing_file_for_correcting_genotypes.py \\\n",
    "../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100.haplotypes.tsv \\\n",
    "../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100_biallelic.catalog.tsv \\\n",
    "../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100_woheader.catalog.snps.tsv \\\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4] create a bash script that will unzip all of the individual .tags.tsv files, and then call Marine's genotypes_verif python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
