{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CRAGIG RUN 1\n",
    "20170202 \n",
    "\n",
    "I got my first batch of RAD data back from BGI, and have yet to really look at it. I'd like to take a first look for population differences, but also I need to help Molly figure out how to build a parentage panel. My first look will be at the **forward reads.**\n",
    "\n",
    "### Download Data from BGI\n",
    "\n",
    "I downloaded the data by clicking on the download FTP button. Downloading each of the two raw data files took about 24 hours, and crashed occasionally. I then moved the two files onto the E drive (which has a ton of space), copied them, and saved on as untouchable. The other, I put in a shared folder so I can play with it in my virtual machine.\n",
    "\n",
    "### Check that Download was Error Free\n",
    "\n",
    "used md5sum, a built in function in Ubuntu, and compared the check sum associated with the raw forward read file and it matched\n",
    "\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder$ md5sum 161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_1.fq.gz\n",
    "\n",
    "6d7714b6f03d65002894ba5d1c16b520  161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_1.fq.gz\n",
    "```\n",
    "\n",
    "While I'm' at it, going to check the reverse reads too:\n",
    "\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/Raw_data$ md5sum 161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_2.fq.gz \n",
    "27ed5a022a51bbc5fa23300bb22e602e  161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_2.fq.gz\n",
    "```\n",
    "\n",
    "### ``process_radtags``\n",
    "\n",
    "``process_radtags`` dumultiplexes your raw data file and (optionally) names the sample by file name. Be sure to include unique file names for any replicates. For example, fg100-fg104, I sequenced twice, each with a unique barcode.\n",
    "\n",
    "\n",
    "[mary says don't use filterillumina; used r c q and trimmed]\n",
    "\n",
    "Stacks [manual page for ``process_radtags``](http://catchenlab.life.illinois.edu/stacks/comp/process_radtags.php)\n",
    "\n",
    "Inputs values I will use\n",
    "\n",
    " f — path to the input file if processing single-end seqeunces \n",
    "<br>**-f /mnt/hgfs/Data_for Analysis/WorkingFolder/161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_1.fq.gz**\n",
    "\n",
    " i — input file type, either 'bustard' for the Illumina BUSTARD format, 'bam', 'fastq' (default), or 'gzfastq' for gzipped FASTQ.\n",
    "<br>**-i gzfastq**\n",
    "\n",
    " y — output type, either 'fastq', 'gzfastq', 'fasta', or 'gzfasta' (default is to match the input file type).\n",
    "<br>**-y gzfastq**\n",
    "\n",
    " o — path to output the processed files.\n",
    "<br>**-o /mnt/hgfs/Data_for Analysis/WorkingFolder/Stacks**\n",
    "\n",
    " t — truncate final read length to this value.\n",
    "<br>**-t 139** (reads are 150 bp, 6 of which are barcode, and trim at least 5, so start with 139. Then run FastQC and see if more trimming is necessary)\n",
    "\n",
    " b — path to a file containing barcodes for this run.\n",
    "<br>**-b /mnt/hgfs/Data_for Analysis/WorkingFolder/barcodes_cragigrun1.txt**\n",
    "\n",
    " r — rescue barcodes and RAD-Tags (this means if barcode is one base off from any barcode you used, Stacks will use some probability function to match to closest barcode and rewrite sequence to reflect that)\n",
    "<br>**-r**\n",
    "\n",
    " c — clean data, remove any read with an uncalled base.\n",
    "<br>**-c**\n",
    "\n",
    " -e [enz], --renz_1 [enz]: provide the restriction enzyme used (cut site occurs on single-end read) <- 'sbfI'\n",
    "<br>**-e sbfI**\n",
    "\n",
    "Your barcode list:\n",
    "\n",
    "Should take the form of barcode \\t samplename \\n\n",
    "\n",
    "Local path for my current barcode list = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Change to appropriate working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Data_for Analysis/WorkingFolder\n"
     ]
    }
   ],
   "source": [
    "cd /mnt/hgfs/Data_for Analysis/WorkingFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!stacks process_radtags \\\n",
    "-f /mnt/hgfs/Data_for_Analysis/WorkingFolder/161228_I137_FCHCYV5BBXX_L5_CHKPE85216120009_1.fq.gz \\\n",
    "-i gzfastq \\\n",
    "-y gzfastq \\\n",
    "-o /mnt/hgfs/Data_for_Analysis/WorkingFolder/Stacks \\\n",
    "-t 139 \\\n",
    "-b /mnt/hgfs/Data_for_Analysis/WorkingFolder/barcodes_cragigrun1.txt \\\n",
    "-r \\\n",
    "-c \\\n",
    "-e sbfI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**``process_radtags`` output:**\n",
    "\n",
    "387289855 total sequences;\n",
    "  64147742 ambiguous barcode drops;\n",
    "  401015 low quality read drops;\n",
    "  42895231 ambiguous RAD-Tag drops;\n",
    "279845867 retained reads.\n",
    "\n",
    "So 72.26% reads retained.\n",
    "\n",
    "Then, I ran a script to count reads in each post-``process_radtags`` file, which Steven gave to Mary who gave to me. Here's the script. A similar version is on her github, [here](https://github.com/mfisher5/mf-fish546-PCod/blob/master/scripts/L1L2_seqCountsgen.py).\n",
    "\n",
    "```\n",
    "import sys\n",
    "lane1 = open(sys.argv[1], \"r\")\n",
    "newshell = open(\"CountFASTQseqs.sh\", \"w\")\n",
    "\n",
    "newshell.write(\"#!/bin/bash\" + \"\\n\\n\")\n",
    "\n",
    "for line in lane1:\n",
    "\tlinelist = line.strip().split()\n",
    "\tfilestring = \"zcat L1L2samples/\" + linelist[0] + \".fq.gz | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' >> FastQsequenceCounts.txt\"\n",
    "\tnewshell.write(filestring + \"\\n\")\n",
    "lane1.close()\n",
    "newshell.close()\n",
    "```\n",
    "\n",
    "I'm still working on getting ``matplotlib`` installed in my virtual machine, but until then, I exported the results to excel, sorted, and picked the ten samples with the most reads out of each population. For Alaska"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Quality check with FastQC\n",
    "\n",
    "Looked at several individual sample FastQC reports and they all look great. Mean average quality score of 40, with little variation, and none dropped below the green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ``ustacks``\n",
    "\n",
    "I used a python script I wrote previously to build the shell script. There was an error where it skipped the first few of my samples and didn't run ``ustacks`` on them, so I need to improve it. Also it would be great to be able to input the different parameters with flags. And my brother showed me a module that helps you do that... so I'm going to try to do that now.\n",
    "\n",
    "Example command to show parameter values:\n",
    "\n",
    "``stacks ustacks -t gzfastq -f ./Stacks/Q334.fq.gz -r -d -o ./Stacks -i 001 -m 10 -M 3 -p 10``\n",
    "\n",
    "\n",
    "Only one sample retained few loci (FG100_A retained only 6) so I'm going to exclude that one from further analyses. All of the rest of the samples had at least 20K, with the vast majority around 25K-30K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pick samples for ``cstacks``\n",
    "\n",
    "I just got clarified from lab mates that we're supposed to use the 10 most sequenced individuals from **each** population. I don't even have 10 from each in this run, so I'll include up to 10 from each population.\n",
    "\n",
    "Sorting my Excel sheet to determine these...\n",
    "\n",
    "These leads me to:\n",
    "Q324\n",
    "Q318\n",
    "Q326\n",
    "Q314\n",
    "Q316\n",
    "Q323\n",
    "Q325\n",
    "Q351\n",
    "Q352\n",
    "Q332\n",
    "Q339\n",
    "Q334\n",
    "Q347\n",
    "Q356\n",
    "Q330\n",
    "Q355\n",
    "Q354\n",
    "FG009\n",
    "FG033\n",
    "FG002\n",
    "FG027\n",
    "FG021\n",
    "FG013\n",
    "FG001\n",
    "FG005\n",
    "FG014\n",
    "FG006\n",
    "FG101_A\n",
    "FG102_A\n",
    "FG100_B\n",
    "FG106\n",
    "FG107\n",
    "FG109\n",
    "FG104_A\n",
    "FG111\n",
    "FG108\n",
    "FG103_A\n",
    "FG210\n",
    "FG205\n",
    "FG211\n",
    "FG212\n",
    "FG201\n",
    "FG209\n",
    "FG206\n",
    "FG204\n",
    "FG202\n",
    "FG203\n",
    "\n",
    "\n",
    "<br><br> There are 47 because the AK population only had 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ``cstacks``\n",
    "\n",
    "\n",
    "Now I need to write the ``cstacks`` shell script to run the program on these ten samples. I used my ``easy_cstacks.py`` script.\n",
    "\n",
    "Code for ``cstacks`` was:\n",
    "\n",
    "```\n",
    "stacks cstacks -b 100 -s Stacks/Q324 -s Stacks/Q318 -s Stacks/Q326 -s Stacks/Q314 -s Stacks/Q316 -s Stacks/Q323 -s Stacks/Q325 -s Stacks/Q351 -s Stacks/Q352 -s Stacks/Q332 -s Stacks/Q339 -s Stacks/Q334 -s Stacks/Q347 -s Stacks/Q356 -s Stacks/Q330 -s Stacks/Q355 -s Stacks/Q354 -s Stacks/FG009 -s Stacks/FG033 -s Stacks/FG002 -s Stacks/FG027 -s Stacks/FG021 -s Stacks/FG013 -s Stacks/FG001 -s Stacks/FG005 -s Stacks/FG014 -s Stacks/FG006 -s Stacks/FG101_A -s Stacks/FG102_A -s Stacks/FG100_B -s Stacks/FG106 -s Stacks/FG107 -s Stacks/FG109 -s Stacks/FG104_A -s Stacks/FG111 -s Stacks/FG108 -s Stacks/FG103_A -s Stacks/FG210 -s Stacks/FG205 -s Stacks/FG211 -s Stacks/FG212 -s Stacks/FG201 -s Stacks/FG209 -s Stacks/FG206 -s Stacks/FG204 -s Stacks/FG202 -s Stacks/FG203 -o Stacks -n 3 -p 3 \n",
    "\n",
    "```\n",
    "Worked! And it took ~6.5 hours to run.\n",
    "\n",
    "```\n",
    "Writing catalog to 'Stacks/... done.\n",
    "\n",
    "Running cstacks took \n",
    "06:27:18.40\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ``sstacks``\n",
    "\n",
    "Need to get all the way through populations in order to form catalog of loci for Bowtie and Blast filtering that will then feed back into the Stacks pipeline starting at ``pstacks``.\n",
    "\n",
    "I wrote a script that makes the ``sstacks`` shell, and runs it. Going to update it and run it. \n",
    "\n",
    "The shell script looks like this:\n",
    "\n",
    "```\n",
    "stacks sstacks -b 100 -s Stacks/Q351 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q352 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q332 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q339 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q334 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q347 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q356 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q330 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q355 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q354 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q338 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q353 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q324 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q318 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q326 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q314 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q316 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q323 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/Q325 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG009 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG033 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG002 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG027 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG021 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG013 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG001 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG005 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG014 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG006 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG004 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG024 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG017 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG003 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG018 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG019 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG016 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG015 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG031 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG032 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG007 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG026 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG011 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG035 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG034 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG010 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG101_A -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG102_A -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG100_B -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG106 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG101_B -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG107 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG109 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG102_B -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG104_A -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG111 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG108 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG103_A -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG103_B -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG104_B -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG100_A -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG210 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG205 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG211 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG212 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG201 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG209 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG206 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG204 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG202 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG203 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "stacks sstacks -b 100 -s Stacks/FG207 -o Stacks/ -p 3 -c Stacks/batch_100 \n",
    "\n",
    "```\n",
    "\n",
    "It worked! And it took 00:13:10.65 to run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Populations\n",
    "\n",
    "#### Question: \n",
    "How should my parameter sets compare between the first round of ``populations`` (to build reference genome) and the second round of ``populations`` when I'm actually comparing populations for popgen?\n",
    "\n",
    "Just talked to Dan, who recommended that I be pretty lenient the first time through, and then very stringent (for Molly's parentage panel needs) the second time through. For lenient, he said an r of 0.25 and for p, about half of the number of populations.\n",
    "\n",
    "Code for this run:\n",
    "\n",
    "```\n",
    "stacks populations -b 100 -P Stacks -M popmap_cragigrun1.txt -t 10 -r 0.25 -p 3 -m 5 --genepop\n",
    "```\n",
    "\n",
    "<br>Here:\n",
    "<br> -b 100 (batch num)\n",
    "<br> -P Stacks (directory w sstacks output files)\n",
    "<br> -M popmap_cragigrun1.txt (pop map)\n",
    "<br> -t 10 (threads)\n",
    "<br> -r .25 (percentage ind in pop required to process locus for pop)\n",
    "<br> -p 3 (min populations loci present in to keep locus)\n",
    "<br> -m 5 specify a minimum stack depth required for individuals at a locus.\n",
    "\n",
    "Output\n",
    "```\n",
    "Writing 28647 loci to summary statistics file, 'Stacks/batch_100.sumstats.tsv'\n",
    "Writing 28647 loci to observed haplotype file, 'Stacks/batch_100.haplotypes.tsv'\n",
    "Writing population data to GenePop file 'Stacks/batch_100.genepop'\n",
    "```\n",
    "\n",
    "Counted loci by columns in pop gen file - 1 (for first column of sample names):\n",
    "\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder$ python count_loci_genepop.py Stacks/batch_100.genepop\n",
    "Your genepop file has 232763 columns of genotypes.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bowtie + BLAST filtering\n",
    "\n",
    "6. Filtering loci with bowtie + BLAST\n",
    "\n",
    "We filter out highly repetive loci using bowtie and BLAST. More information in this markdown file. \n",
    "\n",
    "#### A. Filtering with bowtie\n",
    "Make a fasta file for bowtie by running a custom python script that Mary wrote, which requires (1) a text file with the header from the genepop file and (2) unzipped batch.catalog.tags file.\n",
    "\n",
    "Here, I will make batch_100_loci.txt for this round by manually opening the genepop file in Textwrangler, and cutting and pasting the header line with the tag names into its own text file.\n",
    "\n",
    "first bit of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_11,3_19,3_28,3_38,3_50,3_103,3_130,4_7,4_14,4_39,4_41,4_44,4_46,4_58,4_59,4_61,4_62,4_69,4_76,4_77,4_78,4_85,4_90,4_133,7_20,7_105,7_130,7_131,9_20,9_26,9_36,9_53,9_67,9_70,9_79,9_101,9_117,9_119,9_125,9_137,9_138,10_32,11_96,11_131,12_32,12_50,12_73,12_96,12_98,12_117,12_138,14_8,14_15,14_23,14_29,14_38,14_43,14_44,14_49,14_65,14_67,14_107,16_16,16_46,16_50,16_54,16_73,16_77,16_79,16_97,16_108,16_109,17_29,17_30,17_37,17_48,17_50,17_85,17_89,17_97,17_108,17_113,17_120,18_9,18_13,18_35,18_46,18_53,18_59,18_65,18_70,18_71,18_90,18_91,18_95,18_97,18_99,18_108,18_112,18_114,18_117,18_118,18_119,18_136,23_58,23_118,23_119,23_120,26_49,26_55,26_89,26_90,26_137,27_8,27_28,27_49,27_67,27_76,27_86,27_134,27_135,32_22,32_23,32_35,32_39,32_52,32_75,32_84,32_101,32_120,32_121,32_136,32_137,33_9,33_13,33_33,33_52,33_127,36_16,36_18,36_33,36_45,36_58,36_94,36_116,36_126,36_133,37_10,37_23,37_34,37_67,37_82,37_85,37_109,37_111,37_117,37_118,38_18,38_29,38_32,38_50,43_8,43_21,43_27,43_30,43_32,43_36,43_61,43_74,45_67,46_24,46_41,46_70,46_85,46_96,46_116,46_133,46_134,46_137,50_8,50_34,50_132,52_30,52_37,52_102,52_105,53_7,53_10,53_40,53_42,53_55,53_67,53_70,53_77,53_88,53_91,53_93,53_105,53_109,53_116,53_123,53_138,57_14,57_18,57_24,57_27,57_37,57_38,57_40,57_42,57_45,57_56,57_58,57_63,57_68,57_74,57_76,57_77,57_84,57_85,57_91,57_95,57_101,57_103,57_111,57_119,57_120,57_121,57_131,57_135,57_136,58_21,58_70,58_92,58_112,58_113,58_135,58_136,58_137,62_15,62_39,62_61,62_62,62_75,62_76,64_24,64_50,64_75,64_82,64_98,64_105,64_121,64_136,64_138,66_7,66_34,66_128,67_48,67_56,67_84,67_86,67_88,67_92,67_95,67_116,67_130,67_138,68_29,68_30,68_52,68_96,68_127,71_7,71_12,71_22,71_23,71_49,71_72,71_77,71_80,71_88,71_97,71_107,71_124,72_9,72_10,72_32,72_41,72_57,72_68,72_79,72_81,72_98,72_111,72_124,76_10,76_95,79_30,79_33,79_119,79_135,82_81,82_99,83_34,83_84,83_103,83_125,94_32,94_40,94_52,94_56,94_60,94_72,94_73,94_75,94_102,94_106,94_133,94_134,95_28,95_45,95_85,95_131,95_133,98_22,98_24,\r\n"
     ]
    }
   ],
   "source": [
    "!python get_first_line_genepop.py batch_100 batch_100_loci_names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then I will unzip the catalog tags file with ``gzip -d batch_100.catalog.tags.tsv.gz``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Data_for_Analysis/WorkingFolder/Stacks\n"
     ]
    }
   ],
   "source": [
    "cd Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!gzip -d batch_100.catalog.tags.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And now Mary's script..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python gen_bowtie_fasta.py batch_100_loci_names.txt Stacks/batch_100.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Installed bowtie by downloading the zipped folder, navigating to that folder, and running\n",
    "\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder/Bowtie/bowtie-1.2$ sudo apt install bowtie\n",
    "[sudo] password for nclowell: \n",
    "```\n",
    "\n",
    "Manually made a new folder to hold the bowtie software and files I make with bowtie.\n",
    "\n",
    "Now, make bowtie index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:\n",
      "  Output files: \"batch_100.*.ebwt\"\n",
      "  Line rate: 6 (line is 64 bytes)\n",
      "  Lines per side: 1 (side is 64 bytes)\n",
      "  Offset rate: 5 (one in 32)\n",
      "  FTable chars: 10\n",
      "  Strings: unpacked\n",
      "  Max bucket size: default\n",
      "  Max bucket size, sqrt multiplier: default\n",
      "  Max bucket size, len divisor: 4\n",
      "  Difference-cover sample period: 1024\n",
      "  Endianness: little\n",
      "  Actual local endianness: little\n",
      "  Sanity checking: disabled\n",
      "  Assertions: disabled\n",
      "  Random seed: 0\n",
      "  Sizeofs: void*:8, int:4, long:8, size_t:8\n",
      "Input files DNA, FASTA:\n",
      "  seqsforBOWTIE.fa\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 954860\n",
      "Using parameters --bmax 716145 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 716145 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 1, merged 6; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 477429 (target: 716144)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:01\n",
      "  Sorting block of length 587728\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 587729\n",
      "Getting block 2 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 430345\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 430346\n",
      "Getting block 3 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 291143\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 291144\n",
      "Getting block 4 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 637647\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 637648\n",
      "Getting block 5 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 637536\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 637537\n",
      "Getting block 6 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 477289\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:01\n",
      "Returning block of 477290\n",
      "Getting block 7 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 546629\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 546630\n",
      "Getting block 8 of 8\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 211118\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 211119\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 1123971\n",
      "fchr[G]: 1863730\n",
      "fchr[T]: 2618412\n",
      "fchr[$]: 3819442\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 5885248 bytes to primary EBWT file: batch_100.1.ebwt\n",
      "Wrote 477436 bytes to secondary EBWT file: batch_100.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 3819442\n",
      "    bwtLen: 3819443\n",
      "    sz: 954861\n",
      "    bwtSz: 954861\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 119358\n",
      "    offsSz: 477432\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 8526\n",
      "    numSides: 17052\n",
      "    numLines: 17052\n",
      "    ebwtTotLen: 1091328\n",
      "    ebwtTotSz: 1091328\n",
      "    reverse: 0\n",
      "Total time for call to driver() for forward index: 00:00:04\n",
      "Reading reference sizes\n",
      "  Time reading reference sizes: 00:00:00\n",
      "Calculating joined length\n",
      "Writing header\n",
      "Reserving space for joined string\n",
      "Joining reference sequences\n",
      "  Time to join reference sequences: 00:00:00\n",
      "bmax according to bmaxDivN setting: 954860\n",
      "Using parameters --bmax 716145 --dcv 1024\n",
      "  Doing ahead-of-time memory usage test\n",
      "  Passed!  Constructing with these parameters: --bmax 716145 --dcv 1024\n",
      "Constructing suffix-array element generator\n",
      "Building DifferenceCoverSample\n",
      "  Building sPrime\n",
      "  Building sPrimeOrder\n",
      "  V-Sorting samples\n",
      "  V-Sorting samples time: 00:00:00\n",
      "  Allocating rank array\n",
      "  Ranking v-sort output\n",
      "  Ranking v-sort output time: 00:00:00\n",
      "  Invoking Larsson-Sadakane on ranks\n",
      "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
      "  Sanity-checking and returning\n",
      "Building samples\n",
      "Reserving space for 12 sample suffixes\n",
      "Generating random suffixes\n",
      "QSorting 12 sample offsets, eliminating duplicates\n",
      "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
      "Multikey QSorting 12 samples\n",
      "  (Using difference cover)\n",
      "  Multikey QSorting samples time: 00:00:00\n",
      "Calculating bucket sizes\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:01\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Split 2, merged 6; iterating...\n",
      "  Binary sorting into buckets\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Binary sorting into buckets time: 00:00:00\n",
      "Splitting and merging\n",
      "  Splitting and merging time: 00:00:00\n",
      "Avg bucket size: 545634 (target: 716144)\n",
      "Converting suffix-array elements to index image\n",
      "Allocating ftab, absorbFtab\n",
      "Entering Ebwt loop\n",
      "Getting block 1 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 432195\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 432196\n",
      "Getting block 2 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 527983\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:01\n",
      "Returning block of 527984\n",
      "Getting block 3 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 303252\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 303253\n",
      "Getting block 4 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 653457\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 653458\n",
      "Getting block 5 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 683581\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 683582\n",
      "Getting block 6 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 647813\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:01\n",
      "Returning block of 647814\n",
      "Getting block 7 of 7\n",
      "  Reserving size (716145) for bucket\n",
      "  Calculating Z arrays\n",
      "  Calculating Z arrays time: 00:00:00\n",
      "  Entering block accumulator loop:\n",
      "  10%\n",
      "  20%\n",
      "  30%\n",
      "  40%\n",
      "  50%\n",
      "  60%\n",
      "  70%\n",
      "  80%\n",
      "  90%\n",
      "  100%\n",
      "  Block accumulator loop time: 00:00:00\n",
      "  Sorting block of length 571155\n",
      "  (Using difference cover)\n",
      "  Sorting block time: 00:00:00\n",
      "Returning block of 571156\n",
      "Exited Ebwt loop\n",
      "fchr[A]: 0\n",
      "fchr[C]: 1123971\n",
      "fchr[G]: 1863730\n",
      "fchr[T]: 2618412\n",
      "fchr[$]: 3819442\n",
      "Exiting Ebwt::buildToDisk()\n",
      "Returning from initFromVector\n",
      "Wrote 5885248 bytes to primary EBWT file: batch_100.rev.1.ebwt\n",
      "Wrote 477436 bytes to secondary EBWT file: batch_100.rev.2.ebwt\n",
      "Re-opening _in1 and _in2 as input streams\n",
      "Returning from Ebwt constructor\n",
      "Headers:\n",
      "    len: 3819442\n",
      "    bwtLen: 3819443\n",
      "    sz: 954861\n",
      "    bwtSz: 954861\n",
      "    lineRate: 6\n",
      "    linesPerSide: 1\n",
      "    offRate: 5\n",
      "    offMask: 0xffffffe0\n",
      "    isaRate: -1\n",
      "    isaMask: 0xffffffff\n",
      "    ftabChars: 10\n",
      "    eftabLen: 20\n",
      "    eftabSz: 80\n",
      "    ftabLen: 1048577\n",
      "    ftabSz: 4194308\n",
      "    offsLen: 119358\n",
      "    offsSz: 477432\n",
      "    isaLen: 0\n",
      "    isaSz: 0\n",
      "    lineSz: 64\n",
      "    sideSz: 64\n",
      "    sideBwtSz: 56\n",
      "    sideBwtLen: 224\n",
      "    numSidePairs: 8526\n",
      "    numSides: 17052\n",
      "    numLines: 17052\n",
      "    ebwtTotLen: 1091328\n",
      "    ebwtTotSz: 1091328\n",
      "    reverse: 0\n",
      "Total time for backward call to driver() for mirror index: 00:00:05\n"
     ]
    }
   ],
   "source": [
    "!bowtie-build seqsforBOWTIE.fa batch_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now align sequenecs to the index you just made to identify any sequences that align to multiple places in the genome (maybe highly repetive loci like microsats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reads processed: 27478\r\n",
      "# reads with at least one reported alignment: 27478 (100.00%)\r\n",
      "# reads that failed to align: 0 (0.00%)\r\n",
      "Reported 27478 alignments to 1 output stream(s)\r\n"
     ]
    }
   ],
   "source": [
    "!bowtie -f -v 3 --sam --sam-nohead \\\n",
    "batch_100 \\\n",
    "seqsforBOWTIE.fa \\\n",
    "batch_100_BOWTIEout.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then use Dan's custom script to remove unwanted sequences after bowtie alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bowtie output lines read: 27478 5089t lines read: 8637d: 14625 lines read: 17738owtie output lines read: 20232 lines read: 22858umber of Bowtie output lines read: 25545\n",
      "Number of sequences written to output: 27476ritten to output: 2655written to output: 646nces written to output: 789mber of sequences written to output: 956itten to output: 1166 output: 1352 of sequences written to output: 1562mber of sequences written to output: 1725sequences written to output: 1911s written to output: 2097 2306mber of sequences written to output: 2493sequences written to output: 2679s written to output: 2865n to output: 3051itten to output: 3214 output: 3400n to output: 3563mber of sequences written to output: 3773s written to output: 3889mber of sequences written to output: 40291 of sequences written to output: 4378n to output: 4587mber of sequences written to output: 4797mber of sequences written to output: 5053itten to output: 52621 output: 5704 of sequences written to output: 5914 of sequences written to output: 6170n to output: 6379mber of sequences written to output: 6589mber of sequences written to output: 6845itten to output: 7054 output: 7240 output: 7496 of sequences written to output: 7706 of sequences written to output: 7962 8194ences written to output: 8404mber of sequences written to output: 8637itten to output: 8846itten to output: 91021 output: 9544 9730n to output: 9963ritten to output: 10237ten to output: 10510n to output: 10692o output: 10965output: 11147itten to output: 11352mber of sequences written to output: 11580 to output: 11807s written to output: 12012ritten to output: 12285umber of sequences written to output: 12513nces written to output: 12763o output: 130133umber of sequences written to output: 13537nces written to output: 137874014 sequences written to output: 14265tten to output: 14515ber of sequences written to output: 14743 of sequences written to output: 15016written to output: 15266ber of sequences written to output: 15767es written to output: 16017written to output: 16290put: 16540ences written to output: 16768to output: 1701868equences written to output: 17519en to output: 1776918019f sequences written to output: 18270itten to output: 18520mber of sequences written to output: 18748 to output: 18975 output: 19248 sequences written to output: 19385tten to output: 19635ber of sequences written to output: 19863r of sequences written to output: 20045of sequences written to output: 202271tten to output: 20659ences written to output: 20864to output: 21114 written to output: 21319sequences written to output: 21524quences written to output: 21706ber of sequences written to output: 2191122115 output: 22320tput: 22593t: 22866r of sequences written to output: 23117 written to output: 23367uences written to output: 23845er of sequences written to output: 24050 of sequences written to output: 2423236ten to output: 24846n to output: 25028to output: 252103691put: 25756t: 25938 to output: 26143o output: 26325written to output: 26530tput: 26689ut: 26871ber of sequences written to output: 2703127235417\n"
     ]
    }
   ],
   "source": [
    "!python parseBowtie_DD.py batch_100_BOWTIEout.sam batch_100_BOWTIEout_filtered.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I ran this command in the terminal outside jupyter notebook because the auto updating of the sys.stout.write() was freakin' out the notebook.\n",
    "\n",
    "```\n",
    "python parseBowtie_DD.py batch_100_BOWTIEout.sam batch_100_BOWTIEout_filtered.fa\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "Number of Bowtie output lines read: 27478\n",
    "Number of sequences written to output: 27476\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To see how many loci were filtered out due to Bowtie, first count how many unique loci were in the genepop file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your genepop file has 232763 columns of genotypes.\r\n"
     ]
    }
   ],
   "source": [
    "!python count_loci_genepop.py Stacks/batch_100.genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I think that means Bowtie filtered out a ton of loci. 232763-27478 = 205285 = 88%\n",
    "\n",
    "That's sort of insane compared to when I ran through the Pacific cod data, but I did use extremely lenient parameters on this run of populations.\n",
    "\n",
    "## Filtering with Blast\n",
    "\n",
    "Make a blast database with filtered output from Bowtie filtering, then blast sequences to database and remove any loci that match other loci equally well or better than to themselves. This is supposed to remove highly repetitive loci like microsatellites that can interfere with our data analysis.\n",
    "\n",
    "Need to install Blast, make sure I can run it from anywhere, make a directory for Blast files, move the filtered bowtie fasta there, then blast sequences to the database and filter with Dan's Blast parsing script.\n",
    "\n",
    "First step... install Blast on Ubuntu:\n",
    "\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder$ sudo apt-get install ncbi-blast+\n",
    "```\n",
    "\n",
    "Make sure blast is working:\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder$ blastp\n",
    "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\n",
    "Please refer to the BLAST+ user manual.\n",
    "```\n",
    "Now, make a directory for Blast files.\n",
    "\n",
    "<img src=\"bowtie_folder_made.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Building a new DB, current time: 02/09/2017 11:04:48\n",
      "New DB name:   /mnt/hgfs/Data_for_Analysis/WorkingFolder/Blast/batch_100_BOWTIEfiltered\n",
      "New DB title:  batch_100_BOWTIEout_filtered.fa\n",
      "Sequence type: Nucleotide\n",
      "Keep Linkouts: T\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 27476 sequences in 0.571891 seconds.\n"
     ]
    }
   ],
   "source": [
    "!makeblastdb -in batch_100_BOWTIEout_filtered.fa \\\n",
    "-parse_seqids \\\n",
    "-dbtype nucl \\\n",
    "-out batch_100_BOWTIEfiltered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now blast against itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!blastn -query batch_100_BOWTIEout_filtered.fa \\\n",
    "-db batch_100_BOWTIEfiltered \\\n",
    "-out batch_100_BowtieBlastFiltered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use Dan's python script to filter only wanted loci:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It was crashing the notebook... in the terminal I ran:\n",
    "```\n",
    "nclowell@ubuntu:/mnt/hgfs/Data_for_Analysis/WorkingFolder/Blast$ python checkBlastResults_DD.py \\\n",
    "> batch_100_BowtieBlastFiltered \\\n",
    "> batch_100_BOWTIEout_filtered.fa \\\n",
    "> batch_100_BowtieBlastFiltered_GOOD.fa \\\n",
    "> batch_100_BowtieBlastFiltered_BAD.fa\n",
    "\n",
    "Identifying which loci are 'good' and 'bad' based on BLAST alignments...\n",
    "Writing 'good' and 'bad' loci to their respective files...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, check how many loci were filtered out during the BLAST phase of filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Data_for_Analysis/WorkingFolder/Blast\n"
     ]
    }
   ],
   "source": [
    "cd Data_for_Analysis/WorkingFolder/Blast/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27460\r\n"
     ]
    }
   ],
   "source": [
    "!grep \">\" batch_100_BowtieBlastFiltered_GOOD.fa | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\r\n"
     ]
    }
   ],
   "source": [
    "!grep \">\" batch_100_BowtieBlastFiltered_BAD.fa | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So that makes 27460 - 16 = 27444, or <.1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating final reference genome with Bowtie\n",
    "\n",
    "Lastly, I need to use Bowtie again to build a final Bowtie index using the files cleaned in Blast, and then use Bowtie to align all of my fastq files to the Bowtie index for pstacks. So I manually need to move the output files from Blast to the Bowtie folder. Its these alignments (of each fastq with the final reference genome) that feed into ``pstacks``.\n",
    "\n",
    "Ran this outside the notebook because it crashes the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!bowtie-build batch_100_BowtieBlastFiltered_GOOD.fa batch_100_final_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, need to align each fastq file against final reference genome. I made a Python script to write that bash script. Going to see if it needs updating...\n",
    "\n",
    "Sample bash script line:\n",
    "``bowtie -q -v 3 --norc --sam Blast/batch_100_final_index Stacks/Q351.fq Q351.sam``\n",
    "\n",
    "Output:\n",
    "\n",
    "...\n",
    "```\n",
    "Reported 2500668 alignments to 1 output stream(s)\n",
    "# reads processed: 3153859\n",
    "# reads with at least one reported alignment: 2269973 (71.97%)\n",
    "# reads that failed to align: 883886 (28.03%)\n",
    "Reported 2269973 alignments to 1 output stream(s)\n",
    "\n",
    "Running bowtie took \n",
    "08:10:31.52\n",
    "```\n",
    "\n",
    "Now into ``pstacks``!\n",
    "\n",
    "## ``pstacks``\n",
    "\n",
    "Wrote a ``pstacks`` bash with a script I made last quarter.\n",
    "\n",
    "Sample ``pstacks`` line of code:\n",
    "\n",
    "``stacks pstacks -p 5 -m 10 -t sam -i 1 -f Stacks/Q351.sam -o Stacks``\n",
    "\n",
    "Output:\n",
    "\n",
    "...\n",
    "```\n",
    "Parsing Stacks/FG207.sam\n",
    "  Analyzed 2269973 sequence reads; Identified 146488 unique stacks from those reads.\n",
    "  Merged 149407 unique Stacks into 17980 loci.\n",
    "Identifying polymorphic sites and calling consensus sequences...done.\n",
    "  Number of utilized reads 2269973\n",
    "  Mean coverage depth is 129.1; Std Dev: 243.874; Max: 14096\n",
    "Writing loci, SNPs, alleles to 'Stacks/...'\n",
    "  Wrote 17573 loci, excluded 407 loci due to insuffient depth of coverage; blacklisted 0 loci.\n",
    "Finished running pstacks_shell.txt script.\n",
    "\n",
    "Running pstacks took \n",
    "01:23:37.17\n",
    "\n",
    "```\n",
    "\n",
    "## Second round ``cstacks``\n",
    "\n",
    "I'm going to use my ``easy_cstacks.py`` script. Notice that now we have to tags, snps, and alleles file, one set from ustacks and one now from pstacks. I moved all the pstacks ones into their own directory (called pstacks).\n",
    "\n",
    "This is what the bash script looks like:\n",
    "\n",
    "```\n",
    "Currently, this program thinks you intend to include 47 samples in cstacks.\n",
    "\n",
    "Type YES if correct. Type NO if incorrect and check your files and code.YES\n",
    "\n",
    "Sample number verified. Program continuing.\n",
    "\n",
    "Finished writing cstacks shell script.\n",
    "\n",
    "This is what your cstacks shell looks like.\n",
    "stacks cstacks -b 100 -g -s pstacks/Q324 -s pstacks/Q318 -s pstacks/Q326 -s pstacks/Q314 -s pstacks/Q316 -s pstacks/Q323 -s pstacks/Q325 -s pstacks/Q351 -s pstacks/Q352 -s pstacks/Q332 -s pstacks/Q339 -s pstacks/Q334 -s pstacks/Q347 -s pstacks/Q356 -s pstacks/Q330 -s pstacks/Q355 -s pstacks/Q354 -s pstacks/FG009 -s pstacks/FG033 -s pstacks/FG002 -s pstacks/FG027 -s pstacks/FG021 -s pstacks/FG013 -s pstacks/FG001 -s pstacks/FG005 -s pstacks/FG014 -s pstacks/FG006 -s pstacks/FG101_A -s pstacks/FG102_A -s pstacks/FG100_B -s pstacks/FG106 -s pstacks/FG107 -s pstacks/FG109 -s pstacks/FG104_A -s pstacks/FG111 -s pstacks/FG108 -s pstacks/FG103_A -s pstacks/FG210 -s pstacks/FG205 -s pstacks/FG211 -s pstacks/FG212 -s pstacks/FG201 -s pstacks/FG209 -s pstacks/FG206 -s pstacks/FG204 -s pstacks/FG202 -s pstacks/FG203 -o pstacks -n 3 -p 5 \n",
    "Run this shell script?YES\n",
    "Running shell script.\n",
    "Number of mismatches allowed between stacks: 3\n",
    "Loci matched based on genomic location.\n",
    "Constructing catalog from 47 samples.\n",
    "Initializing new catalog...\n",
    "  Parsing pstacks/Q324.tags.tsv\n",
    "  Parsing pstacks/Q324.snps.tsv\n",
    "  Parsing pstacks/Q324.alleles.tsv\n",
    "Building an index of the catalog.\n",
    "Processing sample pstacks/Q324 [2 of 47]\n",
    "  Parsing pstacks/Q318.tags.tsv\n",
    "  Parsing pstacks/Q318.snps.tsv\n",
    "  Parsing pstacks/Q318.alleles.tsv\n",
    "Searching for matches by genomic location...\n",
    "Merging matches into catalog...\n",
    "  0 loci matched more than one catalog locus and were excluded.\n",
    "  Updating catalog index...\n",
    "Processing sample pstacks/Q318 [3 of 47]\n",
    "  Parsing pstacks/Q326.tags.tsv\n",
    "\n",
    "```\n",
    "\n",
    "The output looks like:\n",
    "\n",
    "...\n",
    "```\n",
    "Processing sample pstacks/FG204 [46 of 47]\n",
    "  Parsing pstacks/FG202.tags.tsv\n",
    "  Parsing pstacks/FG202.snps.tsv\n",
    "  Parsing pstacks/FG202.alleles.tsv\n",
    "Searching for matches by genomic location...\n",
    "Merging matches into catalog...\n",
    "  0 loci matched more than one catalog locus and were excluded.\n",
    "  Updating catalog index...\n",
    "Processing sample pstacks/FG202 [47 of 47]\n",
    "  Parsing pstacks/FG203.tags.tsv\n",
    "  Parsing pstacks/FG203.snps.tsv\n",
    "  Parsing pstacks/FG203.alleles.tsv\n",
    "Searching for matches by genomic location...\n",
    "Merging matches into catalog...\n",
    "  0 loci matched more than one catalog locus and were excluded.\n",
    "  Updating catalog index...\n",
    "Writing catalog to 'pstacks/... done.\n",
    "\n",
    "Running cstacks took \n",
    "00:15:31.07\n",
    "\n",
    "```\n",
    "It ran so fast though - going to double-check that nothing is fishy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_100.catalog.tags.tsv:27460\r\n"
     ]
    }
   ],
   "source": [
    "!grep --count --with-filename consensus batch_100.catalog.tags.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looks like I have 27460 loci in my catalog, that looks about normal (Eleni had 29K loci in her catalog).\n",
    "\n",
    "## Second ``sstacks``\n",
    "\n",
    "First line of the bash script looked like:\n",
    "\n",
    "```\n",
    "stacks sstacks -g -b 100 -s pstacks/Q351 -o pstacks -p 3 -c pstacks/batch_100\n",
    "```\n",
    "\n",
    "Output looked like:\n",
    "\n",
    "...\n",
    "```\n",
    "Processing sample 'pstacks/FG207' [1 of 1]\n",
    "  Parsing pstacks/FG207.tags.tsv\n",
    "  Parsing pstacks/FG207.snps.tsv\n",
    "  Parsing pstacks/FG207.alleles.tsv\n",
    "Searching for matches by genomic location...\n",
    "  Creating map of genomic locations...done.\n",
    "17573 stacks matched against the catalog containing 27460 loci.\n",
    "  17573 matching loci, 1527 contained no verified haplotypes.\n",
    "  1204 loci contained SNPs unaccounted for in the catalog and were excluded.\n",
    "  23178 total haplotypes examined from matching loci, 19817 verified.\n",
    "Outputing to file pstacks/FG207.matches.tsv\n",
    "00:24:00.03\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Second ``populations``\n",
    "\n",
    "``\n",
    "stacks populations -b 100 -P Stacks_2 -M popmap_cragigrun1.txt -t 10 -r 1 -p 5 -m 5 --genepop\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/mnt/hgfs/Data_for_Analysis/WorkingFolder'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fst kernel smoothing: off\n",
      "Bootstrap resampling: off\n",
      "Percent samples limit per population: 1\n",
      "Locus Population limit: 5\n",
      "Minimum stack depth: 10\n",
      "Log liklihood filtering: off; threshold: 0\n",
      "Minor allele frequency cutoff: 0\n",
      "Maximum observed heterozygosity cutoff: 1\n",
      "Applying Fst correction: none.\n",
      "Parsing population map.\n",
      "Found 71 input file(s).\n",
      "  5 populations found\n",
      "    WA_Strait: FG001, FG002, FG003, FG004, FG005, FG006, FG007, FG009, FG010, FG011, FG013, FG014, FG015, FG016, FG017, FG018, FG019, FG021, FG024, FG026, FG027, FG031, FG032, FG033, FG034, FG035\n",
      "    WA_SanJuans: FG100_A, FG100_B, FG101_A, FG101_B, FG102_A, FG102_B, FG103_A, FG103_B, FG104_A, FG104_B, FG106, FG107, FG108, FG109, FG111\n",
      "    WA_Dabob: FG201, FG202, FG203, FG204, FG205, FG206, FG207, FG209, FG210, FG211, FG212\n",
      "    AK: Q314, Q316, Q318, Q323, Q324, Q325, Q326\n",
      "    CA_Catalina: Q330, Q332, Q334, Q338, Q339, Q347, Q351, Q352, Q353, Q354, Q355, Q356\n",
      "  1 group of populations found\n",
      "    1: WA_Strait, WA_SanJuans, WA_Dabob, AK, CA_Catalina\n",
      "  Parsing pstacks/batch_100.catalog.tags.tsv\n",
      "  Parsing pstacks/batch_100.catalog.snps.tsv\n",
      "  Parsing pstacks/batch_100.catalog.alleles.tsv\n",
      "  Parsing pstacks/FG001.matches.tsv\n",
      "  Parsing pstacks/FG002.matches.tsv\n",
      "  Parsing pstacks/FG003.matches.tsv\n",
      "  Parsing pstacks/FG004.matches.tsv\n",
      "  Parsing pstacks/FG005.matches.tsv\n",
      "  Parsing pstacks/FG006.matches.tsv\n",
      "  Parsing pstacks/FG007.matches.tsv\n",
      "  Parsing pstacks/FG009.matches.tsv\n",
      "  Parsing pstacks/FG010.matches.tsv\n",
      "  Parsing pstacks/FG011.matches.tsv\n",
      "  Parsing pstacks/FG013.matches.tsv\n",
      "  Parsing pstacks/FG014.matches.tsv\n",
      "  Parsing pstacks/FG015.matches.tsv\n",
      "  Parsing pstacks/FG016.matches.tsv\n",
      "  Parsing pstacks/FG017.matches.tsv\n",
      "  Parsing pstacks/FG018.matches.tsv\n",
      "  Parsing pstacks/FG019.matches.tsv\n",
      "  Parsing pstacks/FG021.matches.tsv\n",
      "  Parsing pstacks/FG024.matches.tsv\n",
      "  Parsing pstacks/FG026.matches.tsv\n",
      "  Parsing pstacks/FG027.matches.tsv\n",
      "  Parsing pstacks/FG031.matches.tsv\n",
      "  Parsing pstacks/FG032.matches.tsv\n",
      "  Parsing pstacks/FG033.matches.tsv\n",
      "  Parsing pstacks/FG034.matches.tsv\n",
      "  Parsing pstacks/FG035.matches.tsv\n",
      "  Parsing pstacks/FG100_A.matches.tsv\n",
      "Warning: unable to find any matches in file 'FG100_A', excluding this sample from population analysis.\n",
      "  Parsing pstacks/FG100_B.matches.tsv\n",
      "  Parsing pstacks/FG101_A.matches.tsv\n",
      "  Parsing pstacks/FG101_B.matches.tsv\n",
      "  Parsing pstacks/FG102_A.matches.tsv\n",
      "  Parsing pstacks/FG102_B.matches.tsv\n",
      "  Parsing pstacks/FG103_A.matches.tsv\n",
      "  Parsing pstacks/FG103_B.matches.tsv\n",
      "  Parsing pstacks/FG104_A.matches.tsv\n",
      "  Parsing pstacks/FG104_B.matches.tsv\n",
      "  Parsing pstacks/FG106.matches.tsv\n",
      "  Parsing pstacks/FG107.matches.tsv\n",
      "  Parsing pstacks/FG108.matches.tsv\n",
      "  Parsing pstacks/FG109.matches.tsv\n",
      "  Parsing pstacks/FG111.matches.tsv\n",
      "  Parsing pstacks/FG201.matches.tsv\n",
      "  Parsing pstacks/FG202.matches.tsv\n",
      "  Parsing pstacks/FG203.matches.tsv\n",
      "  Parsing pstacks/FG204.matches.tsv\n",
      "  Parsing pstacks/FG205.matches.tsv\n",
      "  Parsing pstacks/FG206.matches.tsv\n",
      "  Parsing pstacks/FG207.matches.tsv\n",
      "  Parsing pstacks/FG209.matches.tsv\n",
      "  Parsing pstacks/FG210.matches.tsv\n",
      "  Parsing pstacks/FG211.matches.tsv\n",
      "  Parsing pstacks/FG212.matches.tsv\n",
      "  Parsing pstacks/Q314.matches.tsv\n",
      "  Parsing pstacks/Q316.matches.tsv\n",
      "  Parsing pstacks/Q318.matches.tsv\n",
      "  Parsing pstacks/Q323.matches.tsv\n",
      "  Parsing pstacks/Q324.matches.tsv\n",
      "  Parsing pstacks/Q325.matches.tsv\n",
      "  Parsing pstacks/Q326.matches.tsv\n",
      "  Parsing pstacks/Q330.matches.tsv\n",
      "  Parsing pstacks/Q332.matches.tsv\n",
      "  Parsing pstacks/Q334.matches.tsv\n",
      "  Parsing pstacks/Q338.matches.tsv\n",
      "  Parsing pstacks/Q339.matches.tsv\n",
      "  Parsing pstacks/Q347.matches.tsv\n",
      "  Parsing pstacks/Q351.matches.tsv\n",
      "  Parsing pstacks/Q352.matches.tsv\n",
      "  Parsing pstacks/Q353.matches.tsv\n",
      "  Parsing pstacks/Q354.matches.tsv\n",
      "  Parsing pstacks/Q355.matches.tsv\n",
      "  Parsing pstacks/Q356.matches.tsv\n",
      "Populating observed haplotypes for 70 samples, 27460 loci.\n",
      "Removed 90 samples from loci that are below the minimum stack depth of 10x\n",
      "Removing 26609 loci that did not pass sample/population constraints... retained 851 loci.\n",
      "Loading model outputs for 70 samples, 851 loci.\n",
      "  Parsing pstacks/FG001.tags.tsv\n",
      "  Parsing pstacks/FG002.tags.tsv\n",
      "  Parsing pstacks/FG003.tags.tsv\n",
      "  Parsing pstacks/FG004.tags.tsv\n",
      "  Parsing pstacks/FG005.tags.tsv\n",
      "  Parsing pstacks/FG006.tags.tsv\n",
      "  Parsing pstacks/FG007.tags.tsv\n",
      "  Parsing pstacks/FG009.tags.tsv\n",
      "  Parsing pstacks/FG010.tags.tsv\n",
      "  Parsing pstacks/FG011.tags.tsv\n",
      "  Parsing pstacks/FG013.tags.tsv\n",
      "  Parsing pstacks/FG014.tags.tsv\n",
      "  Parsing pstacks/FG015.tags.tsv\n",
      "  Parsing pstacks/FG016.tags.tsv\n",
      "  Parsing pstacks/FG017.tags.tsv\n",
      "  Parsing pstacks/FG018.tags.tsv\n",
      "  Parsing pstacks/FG019.tags.tsv\n",
      "  Parsing pstacks/FG021.tags.tsv\n",
      "  Parsing pstacks/FG024.tags.tsv\n",
      "  Parsing pstacks/FG026.tags.tsv\n",
      "  Parsing pstacks/FG027.tags.tsv\n",
      "  Parsing pstacks/FG031.tags.tsv\n",
      "  Parsing pstacks/FG032.tags.tsv\n",
      "  Parsing pstacks/FG033.tags.tsv\n",
      "  Parsing pstacks/FG034.tags.tsv\n",
      "  Parsing pstacks/FG035.tags.tsv\n",
      "  Parsing pstacks/FG100_B.tags.tsv\n",
      "  Parsing pstacks/FG101_A.tags.tsv\n",
      "  Parsing pstacks/FG101_B.tags.tsv\n",
      "  Parsing pstacks/FG102_A.tags.tsv\n",
      "  Parsing pstacks/FG102_B.tags.tsv\n",
      "  Parsing pstacks/FG103_A.tags.tsv\n",
      "  Parsing pstacks/FG103_B.tags.tsv\n",
      "  Parsing pstacks/FG104_A.tags.tsv\n",
      "  Parsing pstacks/FG104_B.tags.tsv\n",
      "  Parsing pstacks/FG106.tags.tsv\n",
      "  Parsing pstacks/FG107.tags.tsv\n",
      "  Parsing pstacks/FG108.tags.tsv\n",
      "  Parsing pstacks/FG109.tags.tsv\n",
      "  Parsing pstacks/FG111.tags.tsv\n",
      "  Parsing pstacks/FG201.tags.tsv\n",
      "  Parsing pstacks/FG202.tags.tsv\n",
      "  Parsing pstacks/FG203.tags.tsv\n",
      "  Parsing pstacks/FG204.tags.tsv\n",
      "  Parsing pstacks/FG205.tags.tsv\n",
      "  Parsing pstacks/FG206.tags.tsv\n",
      "  Parsing pstacks/FG207.tags.tsv\n",
      "  Parsing pstacks/FG209.tags.tsv\n",
      "  Parsing pstacks/FG210.tags.tsv\n",
      "  Parsing pstacks/FG211.tags.tsv\n",
      "  Parsing pstacks/FG212.tags.tsv\n",
      "  Parsing pstacks/Q314.tags.tsv\n",
      "  Parsing pstacks/Q316.tags.tsv\n",
      "  Parsing pstacks/Q318.tags.tsv\n",
      "  Parsing pstacks/Q323.tags.tsv\n",
      "  Parsing pstacks/Q324.tags.tsv\n",
      "  Parsing pstacks/Q325.tags.tsv\n",
      "  Parsing pstacks/Q326.tags.tsv\n",
      "  Parsing pstacks/Q330.tags.tsv\n",
      "  Parsing pstacks/Q332.tags.tsv\n",
      "  Parsing pstacks/Q334.tags.tsv\n",
      "  Parsing pstacks/Q338.tags.tsv\n",
      "  Parsing pstacks/Q339.tags.tsv\n",
      "  Parsing pstacks/Q347.tags.tsv\n",
      "  Parsing pstacks/Q351.tags.tsv\n",
      "  Parsing pstacks/Q352.tags.tsv\n",
      "  Parsing pstacks/Q353.tags.tsv\n",
      "  Parsing pstacks/Q354.tags.tsv\n",
      "  Parsing pstacks/Q355.tags.tsv\n",
      "  Parsing pstacks/Q356.tags.tsv\n",
      "Generating nucleotide-level summary statistics for population 'WA_Strait'\n",
      "Population 'WA_Strait' contained 54 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'WA_SanJuans'\n",
      "Population 'WA_SanJuans' contained 36 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'WA_Dabob'\n",
      "Population 'WA_Dabob' contained 33 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'AK'\n",
      "Population 'AK' contained 21 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'CA_Catalina'\n",
      "Population 'CA_Catalina' contained 34 incompatible loci -- more than two alleles present.\n",
      "Tallying loci across populations...done.\n",
      "Pruned 277 variant sites due to filter constraints.\n",
      "  (enable the --verbose flag to record the reason why each site was filtered in the batch_X.populations.log file.)\n",
      "Removing 4 additional loci for which all variant sites were filtered... retained 847 loci.\n",
      "Regenerating nucleotide-level summary statistics for population 'WA_Strait'\n",
      "Population 'WA_Strait' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'WA_SanJuans'\n",
      "Population 'WA_SanJuans' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'WA_Dabob'\n",
      "Population 'WA_Dabob' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'AK'\n",
      "Population 'AK' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'CA_Catalina'\n",
      "Population 'CA_Catalina' contained 0 incompatible loci -- more than two alleles present.\n",
      "Re-tallying loci across populations...done.\n",
      "Generating haplotype-level summary statistics for population 'WA_Strait'\n",
      "Generating haplotype-level summary statistics for population 'WA_SanJuans'\n",
      "Generating haplotype-level summary statistics for population 'WA_Dabob'\n",
      "Generating haplotype-level summary statistics for population 'AK'\n",
      "Generating haplotype-level summary statistics for population 'CA_Catalina'\n",
      "Writing 847 loci to summary statistics file, 'pstacks/batch_100.sumstats.tsv'\n",
      "Writing 847 loci to observed haplotype file, 'pstacks/batch_100.haplotypes.tsv'\n",
      "Writing population data to GenePop file 'pstacks/batch_100.genepop'\n"
     ]
    }
   ],
   "source": [
    "!stacks populations -b 100 -P pstacks -M popmap_cragigrun1.txt -t 10 -r 1 -p 5 -m 10 --genepop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your genepop file has 5027 columns of genotypes.\r\n"
     ]
    }
   ],
   "source": [
    "!python count_loci_genepop.py pstacks/batch_100.genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Filtering shenanigans\n",
    "\n",
    "Everyone in our lab uses a whole bunch of filtering scripts that I haven't used before, so I'll be trying to make sense of them here.\n",
    "\n",
    "Looking at Mary's jupyter [notebook](https://github.com/mfisher5/mf-fish546-PCod/blob/master/notebooks/Lanes%201%20and%202%20combined%20pipeline.ipynb) on this topic for guidance, and Eleni's Evernote page about filtering.\n",
    "\n",
    "#### [1] Remove heading from catalog SNPs file\n",
    "\n",
    "I think Mary does this manually. Going to look at the file and see if I can sort out why... looks like it's just that commented out line. I wrote a short script to remove the first n lines, and removed the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cstacks version 1.35; catalog generated on 2017-02-14 10:51:19\r\n",
      "0\t100\t1\t42\tE\t0\tT\tA\t-\t-\r\n",
      "0\t100\t1\t126\tE\t0\tT\tA\t-\t-\r\n",
      "0\t100\t1\t127\tE\t0\tT\tA\t-\t-\r\n",
      "0\t100\t1\t128\tE\t0\tA\tT\t-\t-\r\n",
      "0\t100\t1\t129\tE\t0\tT\tA\t-\t-\r\n",
      "0\t100\t1\t130\tE\t0\tT\tA\t-\t-\r\n",
      "0\t100\t2\t57\tE\t0\tC\tT\t-\t-\r\n",
      "0\t100\t2\t127\tE\t0\tT\tC\t-\t-\r\n",
      "0\t100\t3\t10\tE\t0\tT\tC\t-\t-\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Your text file has 230573 lines, and you want to extract lines 1 and write it to the file with name ../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100_woheader.catalog.snps.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!python remove_first_n_lines_textfile.py ../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100.catalog.snps.tsv 1 ../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100_woheader.catalog.snps.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Data_for_Analysis/WorkingFolder/pstacks\n"
     ]
    }
   ],
   "source": [
    "cd Data_for_Analysis/WorkingFolder/pstacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t100\t1\t42\tE\t0\tT\tA\t-\t-\r\n",
      "\r\n",
      "0\t100\t1\t126\tE\t0\tT\tA\t-\t-\r\n",
      "\r\n",
      "0\t100\t1\t127\tE\t0\tT\tA\t-\t-\r\n",
      "\r\n",
      "0\t100\t1\t128\tE\t0\tA\tT\t-\t-\r\n",
      "\r\n",
      "0\t100\t1\t129\tE\t0\tT\tA\t-\t-\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head batch_100_woheader.catalog.snps.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### [2] add a \"_\" between Cat and ID in the batch_1 haplotypes.tsv file\n",
    "\n",
    "I think Mary did this manually again. So first unzip, then manually open in a text editor, and add an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: batch_100.haplotypes.tsv: unknown suffix -- ignored\r\n"
     ]
    }
   ],
   "source": [
    "!gzip -d batch_100.haplotypes.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Double-check that the haplotypes file looks good. It does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog_ID\tCnt\tFG001\tFG002\tFG003\tFG004\tFG005\tFG006\tFG007\tFG009\tFG010\tFG011\tFG013\tFG014\tFG015\tFG016\tFG017\tFG018\tFG019\tFG021\tFG024\tFG026\tFG027\tFG031\tFG032\tFG033\tFG034\tFG035\tFG100_B\tFG101_A\tFG101_B\tFG102_A\tFG102_B\tFG103_A\tFG103_B\tFG104_A\tFG104_B\tFG106\tFG107\tFG108\tFG109\tFG111\tFG201\tFG202\tFG203\tFG204\tFG205\tFG206\tFG207\tFG209\tFG210\tFG211\tFG212\tQ314\tQ316\tQ318\tQ323\tQ324\tQ325\tQ326\tQ330\tQ332\tQ334\tQ338\tQ339\tQ347\tQ351\tQ352\tQ353\tQ354\tQ355\tQ356\r\n",
      "119\t70\tCGTACAGT/CGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT/TGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGG/CGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCATATAGT/CGTATAGG\tCGTATAGT\tCGTATAGT/TGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATACT/CGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGCATTGT/CGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTACAGT/CGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTACAGT/CGTATAGT\tCGTATAGT/CGTGTAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\tCGTATAGT\r\n",
      "182\t70\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/TCACTACGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGAATCCGGCGAGC/CCATTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGTTC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGTTC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCCGGGGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGAATCCGGCGAGC/CCACTCCGGGGCCGGCGAGA\tCCACTCAGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/TCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGAATCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGAATCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGTTC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGTCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGTCGGCGAGC\tCCACGCCGGGGCCGGCGAGC/CCGCTCCGGGGCCCGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACGCCGGGGCCGGCGAGC/CCGCTCCGGGGCCCGCGAGC\tCCACTCCGGGGCCGGCGAGC/TCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/TCACTCCGGGGCCGGCGAGC\tCCACTCAGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCAGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCAACTCCGGGGCCGGCGATC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCGCTCCGGGGCCCGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/TCACTACGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCGGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCACTCCGGGGCGGGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGAATCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGA/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCAGGGCCGGCGAGC/CCACTCCGAATCCGGCGAGC\tCCACTCCGGGGCAGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACTACGGGGCCGGCGAGC/TCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCAAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGTTC/CCGCTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGACGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCAGGGGCCGGCGAGC/CCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC/CCGCTCCGGGGCCGGCGAGC\tCCACTCCGGGGCCGGCGAGC\r\n",
      "190\t70\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG/GTG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tACG/ATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG/GTG\tATG\tATA/ATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATA/ATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tATG\tACG/ATG\tATG\tATG\tATG\tATG\tATG\tATG\r\n",
      "196\t70\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG/T\tG/T\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG\tG/T\tG\tG\tG\tG\tG\tG\tG/T\tG\tG\tG\tG\r\n",
      "251\t70\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA/ACCC\tACCA\tACCA\tACCA\tACCA\tACCA/GCCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA/ACGA\tACCA/GCCA\tACCA\tACCA/GCCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA/ATCA\tACCA/ATCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA/ATCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA/ATCA\tACCA\tACCA\tACCA\tACCA\tACCA/GCCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\tACCA\r\n",
      "253\t70\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC/TTC\tGTC\tGTC\tGTC/TTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC/TTC\tGTC/TTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC/GTT\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC/TTC\tGTC\tGTC\tGTC\tGTC\tGAC/GTC\tGTC\tGTC\tGTC/TTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\tGTC\r\n",
      "256\t70\tTGG\tCGG/TGG\tCGG/TGG\tTGG\tCGG/TGG\tCGG/TGG\tCGG\tCGG\tCGG/TGG\tCGG\tTGG\tCGA/CGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tTGG\tCGG/TGG\tTGG\tCGG/TGG\tTGG\tCGG\tCGG/TGG\tCGG\tCGG/TGG\tTGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG\tCGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG\tCGG/TGG\tCGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG\tCGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tTGG\tTGG\tCTG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG/TGG\tCGG\tTGG\tCGG\tCGG/TGG\tCGG\tCGG/TGG\r\n",
      "303\t70\tGACG\tAACG/GACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tAACG\tGACG\tGACG\tAACG/GACG\tGACG/GATG\tGACG/GATG\tGACG\tAACG/GACG\tGACA/GACG\tGACG\tGACG\tAACG/GACG\tGACG\tGACG\tAACG/GACG\tAACG/GACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tAACG/GACG\tGACG\tGACG\tGACA\tGACG\tGACG\tGACG/GGCG\tGACA\tGACG\tGACG/GATG\tAACG/GACG\tAACG/GACG\tGACG\tGACG\tGACG\tGACG\tGACG\tAACG/GACG\tGACG\tGACG\tGACG\tGACG\tGACA/GACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACG\tGACA/GACG\tAACG\r\n",
      "352\t70\tAGGACAAA/ATGACAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA/AGGGCAAA/TGGAGAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA\tAGGACAAA\tAGGACCAA\tAGGACAAA\tAGGGCAAA/ATGACAAA\tAGGACAAA\tAGGACAAA/TTGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA\tAGGACAAA\tATGACAAA/TTGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA\tATGACAAA\tAGGACAAA/AGGGCAAA\tAGGGCAAA/ATGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA/AGGGCAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA\tAGGACAAA/AGGGCAAC\tAGGGCAAA/TTGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA\tAGGACAAA\tAGGGCAAA/ATGACAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tAGAACAAA/AGGACAAA/AGGGCAAA\tAGGACAAA\tAGGGCAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA/TTGACAAA\tAGGGCAAA/ATGACAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA\tAGGACAAA/ATGACAAA\tTGGAGAAA/TTGACAAA\tAGGGCAAA/ATGACAGA\tAGGACAAA\tAGGACAAA\tATGACAAA\tAGGACAAA/ATGACAAA\tAGGACAAA\tAGGGCAAA/ATGACAAC\r\n"
     ]
    }
   ],
   "source": [
    "!head batch_100.haplotypes.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### [3] Run Marine's script to make biallelic catalog reference.\n",
    "\n",
    "Here's a [link to the script](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Scripts/preparing_file_for_correcting_genotypes.py.txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/Ubuntu_Bioinformatics/Git_repo/CRAGIG_run1/Scripts\n"
     ]
    }
   ],
   "source": [
    "cd /mnt/hgfs/Ubuntu_Bioinformatics/Git_repo/CRAGIG_run1/Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 70 CGTACAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT TGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGG CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CATATAGT CGTATAGG CGTATAGT CGTATAGT TGTATAGT CGTATAGT CGTATAGT CGTATACT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGCATTGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTACAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTACAGT CGTATAGT CGTATAGT CGTGTAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT CGTATAGT\r\n",
      "182 70 CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC TCACTACGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCATTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGTTC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGTTC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGGGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCACTCCGGGGCCGGCGAGA CCACTCAGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC TCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGTTC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGTCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGTCGGCGAGC CCACGCCGGGGCCGGCGAGC CCGCTCCGGGGCCCGCGAGC CCGCTCCGGGGCCGGCGAGC CCACGCCGGGGCCGGCGAGC CCGCTCCGGGGCCCGCGAGC CCACTCCGGGGCCGGCGAGC TCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC TCACTCCGGGGCCGGCGAGC CCACTCAGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCAGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CAACTCCGGGGCCGGCGATC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCGCTCCGGGGCCCGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC TCACTACGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCGCTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCGGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCGCTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCGGGCGAGC CCGCTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGA CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCAGGGCCGGCGAGC CCACTCCGAATCCGGCGAGC CCACTCCGGGGCAGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCGCTCCGGGGCCGGCGAGC CCACTACGGGGCCGGCGAGC TCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCAAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGTTC CCGCTCCGGGGCCGGCGAGC CCACTCCGGGGCCGACGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCAGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC CCGCTCCGGGGCCGGCGAGC CCACTCCGGGGCCGGCGAGC\r\n",
      "190 70 ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG GTG ATG ATG ATG ATG ATG ATG ATG ACG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG GTG ATG ATA ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATA ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ATG ACG ATG ATG ATG ATG ATG ATG ATG\r\n",
      "196 70 G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G T G T G G G G G G G G G G G G G G G G G G G G G G G G G G T G G G G G G G T G G G G\r\n",
      "251 70 ACCA ACCA ACCA ACCA ACCA ACCA ACCC ACCA ACCA ACCA ACCA ACCA GCCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACGA ACCA GCCA ACCA ACCA GCCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ATCA ACCA ATCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ATCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ATCA ACCA ACCA ACCA ACCA ACCA GCCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA ACCA\r\n",
      "253 70 GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC TTC GTC GTC GTC TTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC TTC GTC TTC GTC GTC GTC GTC GTC GTC GTC GTC GTT GTC GTC GTC GTC GTC GTC GTC TTC GTC GTC GTC GTC GAC GTC GTC GTC GTC TTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC GTC\r\n",
      "256 70 TGG CGG TGG CGG TGG TGG CGG TGG CGG TGG CGG CGG CGG TGG CGG TGG CGA CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG TGG CGG TGG TGG CGG TGG TGG CGG CGG TGG CGG CGG TGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG CGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG CGG TGG CGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG CGG CGG TGG CGG TGG CGG TGG TGG TGG CTG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG TGG CGG CGG TGG CGG CGG TGG\r\n",
      "303 70 GACG AACG GACG GACG GACG GACG GACG GACG GACG GACG GACG GACG GACG GACG AACG GACG GACG AACG GACG GACG GATG GACG GATG GACG AACG GACG GACA GACG GACG GACG AACG GACG GACG GACG AACG GACG AACG GACG GACG GACG GACG GACG GACG GACG GACG AACG GACG GACG GACG GACA GACG GACG GACG GGCG GACA GACG GACG GATG AACG GACG AACG GACG GACG GACG GACG GACG GACG AACG GACG GACG GACG GACG GACG GACA GACG GACG GACG GACG GACG GACG GACG GACG GACG GACG GACA GACG AACG\r\n",
      "352 70 AGGACAAA ATGACAAA AGGACAAA AGGACAAA AGGACAAA AGGGCAAA TGGAGAAA AGGACAAA AGGACAAA ATGACAAA AGGACAAA ATGACAAA AGGACAAA ATGACAAA AGGACAAA AGGACAAA AGGACCAA AGGACAAA AGGGCAAA ATGACAAA AGGACAAA AGGACAAA TTGACAAA AGGACAAA AGGGCAAA AGGACAAA AGGACAAA ATGACAAA TTGACAAA AGGACAAA AGGGCAAA AGGACAAA ATGACAAA AGGACAAA ATGACAAA AGGACAAA AGGGCAAA AGGACAAA ATGACAAA AGGACAAA AGGGCAAA AGGGCAAA ATGACAAA AGGACAAA AGGGCAAA AGGACAAA AGGGCAAA AGGACAAA AGGACAAA AGGACAAA ATGACAAA AGGACAAA ATGACAAA AGGACAAA AGGGCAAA AGGACAAA AGGGCAAA AGGACAAA AGGACAAA AGGACAAA AGGACAAA AGGGCAAA AGGACAAA AGGACAAA ATGACAAA AGGACAAA AGGACAAA AGGGCAAC AGGGCAAA TTGACAAA AGGACAAA ATGACAAA AGGACAAA AGGACAAA AGGGCAAA ATGACAAA AGGACAAA AGGACAAA AGGACAAA ATGACAAA AGAACAAA AGGACAAA AGGGCAAA AGGACAAA AGGGCAAA AGGACAAA AGGACAAA ATGACAAA AGGACAAA TTGACAAA AGGGCAAA ATGACAAA AGGACAAA AGGACAAA AGGACAAA AGGACAAA ATGACAAA TGGAGAAA TTGACAAA AGGGCAAA ATGACAGA AGGACAAA AGGACAAA ATGACAAA AGGACAAA ATGACAAA AGGACAAA AGGGCAAA ATGACAAC\r\n",
      "357 70 CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAATA CACAGTA CACAGTA CACAGCG CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA TACAGTA CACAGTA CACAGTA CACAGTA CACAGCG CACAGTA CACAGTG CACAGCA CACAGCG CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CAGAGTA CACAGTA CACAGTA CGCAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACTGTA CACAGTA TACAGTA CACAGTA CACAGTA CACAGTA CACAGCA CACAGCG CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA CACAGCG CACAGTA CACAGTA CACAGTA CACAGTA CACAGTA\r\n",
      "401 70 GTATT GGATT GTATT GTATT GTAAT GTATT GTATT GGATT GTATT GGATT GTATT GTTTT GGATT GTATT GTATT GTATT GGATT GTATT GTATT GTATT GTATA GTATT GTATT GTATT GTATT GGATT GTATT GTATT GTATT GTAAT GTATT GTATT GGATT GTATT GGATT GTATT GTATT GGATT GGATT GTATT GTATT GTATT GTATT GGATT GTATT GGATT GTATT CTATT GTATT GTATT GTATT GGATT GTATT GGATT GTATT GGATT GTATT GTATT GTATT GTATT GTATT GGATT GTATT GTATT GTATT GTATT GGATT GTATT GTATT GGATT GTATT GTATT GGATT GTATT GGATT GTATT GGATT GTAAT GTATT GGATT GTATA GTATT GTATT GGATT GTAAT GGATT GTATT GGATT GTATT GTATT GTATT GTATT GTATT GGATT GTATT\r\n",
      "471 70 GACAA GACAA GACAA GACAA GACCA GACAA GACCA GACAA GACAA GACAA GACAA GACAA GACAA TACAA GACAA GACAA GACAA GACAA GACCA GACAA GACCA GACAA GACAA GACCA GACCA GACCA GACAA GACAA GACAA GACAA GACCA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACCA GACAA GACAA GACAA GACAC GACAA GACCA GACAA GACAA GACAA GACAA GACCA GACAA GACAA GACAA GACCA GACAA GACAA GACAA GACAA GACAA GATAA GACAA GACAA GACAA GACAA GACAA GACAA GACAA GACAC GACAA GACCA GACAA GACCA GACAA GACAA GACAA GACAA GACAA GACAA GACCA GTCAA\r\n",
      "476 70 GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA AA GC GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA GA\r\n",
      "555 70 GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT TTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GGG GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT GTT\r\n",
      "572 70 CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATAACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATAACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCGC CGGGTATGACCAC CGGGTATGAGCAC CGGGTACGACCAC CGGGTATGACCAC CGGCTATGACCAC CGGGTTTGACCAC CGGGTATGACCAC CGGGTATGACCAT CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTTTGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGAGCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAT CGGGTACGACCAC CTGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATAACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGTGTATGATCAC CGGGTATGACCAC CGTGTATGATCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATAACCAC CGGGTATGACCAC CGGGTATGACCAC TGGGTATGACCAC CGGGTATGGCCAC CGGGTATGACCAC CGGGTTTGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTTTGACCAC CGGGTATGACCAC CGGGTATGACCAT CGGGTATGACCAC CGGGGATGACCAT CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACGAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATAACCAC CGGGTTTGACCAC CGGCTATGACCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTTTGACCAC CTGGTATGACCAC CGGGTATGACCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGAGCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTACGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC CGGGTATGACCAC\r\n",
      "588 70 GGGTGCTCCCTACTG GTGTTCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGGTGCTCCCTACTG GGCTGCTCCATACTG GGGTGTTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTA GGGTGCTCCCTACTG GGGTGCTCCCTACAG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTTCCTAATG AGGTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCATACTG GGGTGCTCTCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGTTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCCCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTTCTG GGGTGCTCCCTACTG GGGTGCTCCCTACAG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCGACTG GGCTGCTCCCGACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCATACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCATACTG GGGTGCTCCCTACTG GGGTGCTCCCTACAG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGTTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGTTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGAGCTCCCTACTG GGGTGCTCCCTACTG GGCTGCTCCATACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGGTGCTTCCTAATG GGGTGCTTCCTACTG GGGTGCTCCCTACTG GGGTGCTCTCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTGCTCCCTACTG GGGTACTCCCTACTG GGGTGCTCCCTACTG\r\n",
      "625 70 AG TT TT AG TT TT AG TT TT TT TT AG TT TT TT TT TT TT AG TT AG TT AG TT TT AG TG TT TT AG TT TT TT AG TT TT TT TT TT TT TT TT TT AG TT AG TT TT AG TT TT AG TT TT TT AG TT TT AG TT AG TT AG TT AG TG TT TT AG TT TT TT AG AG TT AG TT TT AG TT AG AG AG TT TT AG TT TT TT TT AG TT TT AG TT\r\n",
      "654 70 TAGAG TAGAG TAGAA TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGTG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG CAGAG TAGAG TAGAG TAGAG TAGAG TTGAG TAGAG TAGAG TAAAG TAGAG TAGAG TAGAG TAGAA TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG TAGAG\r\n",
      "659 70 AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AACC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATT AACC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AATC AACC AATC AATC AATC ATTC AATC AATT AACC AATC AATC AACC AATC AATC AATC AATC AATC AATC AATC ATTC AATC AATC AATC AATC AATC AATC AATC GATC AATC AATC AATC AATC AATC AATC AACC AATC AATC AATC\r\n"
     ]
    }
   ],
   "source": [
    "!python preparing_file_for_correcting_genotypes.py \\\n",
    "../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100.haplotypes.tsv \\\n",
    "../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100_biallelic.catalog.tsv \\\n",
    "../../../../Data_for_Analysis/WorkingFolder/pstacks/batch_100_woheader.catalog.snps.tsv \\\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### [4] Call Marine's genotypes_verif python script\n",
    "\n",
    "Mary wrote a new version that takes arguments instead of having to change the code in the script. I updated the script to manage command line arguments with ``argparse``\n",
    "\n",
    "Here's what the help file looks like:\n",
    "\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/images_for_notebook/corgen_helpfile.png?raw=true)\n",
    "\n",
    "<br>\n",
    "Here's what the command looks like:\n",
    "``python genotypes_verif_v2_no_ref_takeARGS.py -i /mnt/hgfs/Data_for_Analysis/WorkingFolder/pstacks -p /mnt/hgfs/Data_for_Analysis/WorkingFolder/popmap_cragigrun1.txt -b 100 -m 10 -c /mnt/hgfs/Data_for_Analysis/WorkingFolder/pstacks/batch_100_biallelic.catalog.tsv -o batch_100_corgen_bi.txt``\n",
    "\n",
    "<br>And I ran it through the command line outside this notebook, and this is what it looked like:\n",
    "<br>\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/images_for_notebook/corgen_running.png?raw=true)\n",
    "\n",
    "<br> So it prints the name of each file as it is processed.\n",
    "\n",
    "<br> Double-check that the text file wrote..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t10080\t10144\t10166\t10171\t1029\t10339\t10547\t10604\t10609\t10685\t10836\t11456\t11735\t1188\t1189\t12315\t12412\t12420\t12427\t12512\t12577\t12800\t12807\t12888\t12916\t12953\t13236\t13248\t13280\t13669\t14181\t14260\t14367\t14676\t15003\t15081\t15181\t15213\t15267\t15354\t15384\t15467\t15468\t15469\t15474\t1552\t15540\t15541\t15567\t15698\t15863\t1623\t16231\t16269\t16432\t16434\t16435\t16505\t16585\t16669\t16845\t17010\t17132\t17179\t1785\t17873\t18052\t18083\t1809\t18511\t18551\t18665\t18773\t18807\t18900\t19132\t19558\t196\t19639\t19770\t19771\t19884\t19893\t20183\t20333\t20574\t20582\t20615\t20616\t20669\t20706\t20741\t20843\t20981\t21054\t21154\t21171\t21372\t21424\t21465\t21593\t21844\t2189\t21965\t22140\t22391\t22536\t22629\t2400\t2481\t253\t256\t2717\t2732\t2796\t2884\t3068\t3313\t3349\t3421\t3521\t3580\t3804\t3864\t3931\t4407\t4449\t4463\t4516\t4776\t4814\t4836\t4969\t5148\t5250\t5350\t5612\t5723\t654\t6868\t6943\t7129\t7132\t7259\t7369\t7379\t7435\t7531\t7672\t8140\t825\t827\t8287\t8456\t8460\t8464\t8677\t9298\t9368\t9511\t9536\t969\t9865\t\r\n",
      "FG009\ta\ta\tb\tb\tb\ta\tab\tb\tb\t-\tab\tb\t-\tb\ta\tb\ta\ta\tab\ta\tb\tb\tab\ta\tab\tb\ta\tab\tb\tb\tb\tb\t-\t-\tb\tb\ta\tb\tb\ta\ta\tab\tab\tab\tab\tb\tab\tab\tab\tb\tab\tab\ta\ta\tab\tab\tab\tab\ta\tb\ta\ta\ta\tab\tb\tb\tab\tb\tb\ta\tab\ta\ta\tb\tab\tb\ta\ta\tb\ta\tb\tab\tb\tb\tb\tb\ta\ta\tb\tb\tb\tb\tb\tb\tb\tb\tb\tb\ta\tab\tb\ta\ta\t-\ta\tb\ta\tb\ta\ta\ta\ta\ta\ta\tb\tb\tb\tab\ta\ta\ta\tb\ta\tb\ta\tb\tab\tab\tab\ta\ta\tb\tb\tb\tab\tb\tab\tb\tb\tb\t-\tab\ta\tb\tab\ta\tb\tb\ta\ta\tab\tb\tb\tb\tb\ta\tab\ta\tab\t-\tb\tb\ta\t\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 batch_100_corgen_bi.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ruh roh! It looks like there are super few loci. This (above) is the first two lines of the text file, where the first is the numbered loci, and the second is the genotypes in letter format for the first individual. However! Mary has a ton more loci. I know that I used incredibly stringent filtering, so maybe that's why. I should just quickly double-check the number of loci that came out of populations round 2.\n",
    "\n",
    "<br>Oh snikies, there were 5027 loci at the end of populations round 2.\n",
    "\n",
    "Chatting with Dan, Lorenz, Mary, and hopefully Brent soon... I'm realizing that this is probably real and that I probably shouldn't be filtering for biallelic loci. Lorenz said it would be more powerful to use multi-allelic in a parentage panel.\n",
    "\n",
    "He also said that if I want to make sure this result isn't crazy, I should try plotting number of SNPs by number of haplotypes, and also look to see if loci within a tag are in in HWE.\n",
    "\n",
    "\n",
    "20170302\n",
    "\n",
    "I had a chat with Lorenz, Brent, and Molly this week that resteered my bioinformatics plan a bit. First, Lorenz pointed out that running through the bioinformatics for population structure shouldn't be that different than finding loci for Molly's parentage panel. Lorenz suggested that I rerun ``populations`` a second time, keeping loci that occur in 75% or more of individuals, in all populations. He also said to not filter for biallelic loci, and also to filter for a minor allele frequency of .02. He also said that I should go forward with a few ways of visualizing my data before deciding a course of action, and these included checking for HWE at each locus and looking at FIS distribution to try to weed out things like paralogous genes.\n",
    "\n",
    "This reorganization means I should probably name some files differently. So I'm going to lump all the ``populations`` output that I had into a new folder, and put the date that I ran it on. And I'm also going to rename the poorly named  ``pstacks`` folder as ``Stacks-2`` for the second round of Stacks after we've made our de novo reference \"genome.\" \n",
    "\n",
    "I'm going to keep everything together in a folder with the date I ran ``populations`` because I can't rename the batch number apparently (it looks for a catalog with that name).\n",
    "\n",
    "## ``Populations`` Round 2, take 2\n",
    "\n",
    "Going to use a -p of 5 because Lorenz told me to, and an -r of .75 is what Lorenz suggested. I think I could even go a little lower if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/SHARED_FOLDER/WorkingFolder/Stacks_2\n"
     ]
    }
   ],
   "source": [
    "cd /mnt/hgfs/SHARED_FOLDER/WorkingFolder/Stacks_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fst kernel smoothing: off\n",
      "Bootstrap resampling: off\n",
      "Percent samples limit per population: 0.75\n",
      "Locus Population limit: 5\n",
      "Minimum stack depth: 10\n",
      "Log liklihood filtering: off; threshold: 0\n",
      "Minor allele frequency cutoff: 0\n",
      "Maximum observed heterozygosity cutoff: 1\n",
      "Applying Fst correction: none.\n",
      "Parsing population map.\n",
      "Found 71 input file(s).\n",
      "  5 populations found\n",
      "    WA_Strait: FG001, FG002, FG003, FG004, FG005, FG006, FG007, FG009, FG010, FG011, FG013, FG014, FG015, FG016, FG017, FG018, FG019, FG021, FG024, FG026, FG027, FG031, FG032, FG033, FG034, FG035\n",
      "    WA_SanJuans: FG100_A, FG100_B, FG101_A, FG101_B, FG102_A, FG102_B, FG103_A, FG103_B, FG104_A, FG104_B, FG106, FG107, FG108, FG109, FG111\n",
      "    WA_Dabob: FG201, FG202, FG203, FG204, FG205, FG206, FG207, FG209, FG210, FG211, FG212\n",
      "    AK: Q314, Q316, Q318, Q323, Q324, Q325, Q326\n",
      "    CA_Catalina: Q330, Q332, Q334, Q338, Q339, Q347, Q351, Q352, Q353, Q354, Q355, Q356\n",
      "  1 group of populations found\n",
      "    1: WA_Strait, WA_SanJuans, WA_Dabob, AK, CA_Catalina\n",
      "  Parsing ../Stacks_2/batch_100.catalog.tags.tsv\n",
      "  Parsing ../Stacks_2/batch_100.catalog.snps.tsv\n",
      "  Parsing ../Stacks_2/batch_100.catalog.alleles.tsv\n",
      "  Parsing ../Stacks_2/FG001.matches.tsv\n",
      "  Parsing ../Stacks_2/FG002.matches.tsv\n",
      "  Parsing ../Stacks_2/FG003.matches.tsv\n",
      "  Parsing ../Stacks_2/FG004.matches.tsv\n",
      "  Parsing ../Stacks_2/FG005.matches.tsv\n",
      "  Parsing ../Stacks_2/FG006.matches.tsv\n",
      "  Parsing ../Stacks_2/FG007.matches.tsv\n",
      "  Parsing ../Stacks_2/FG009.matches.tsv\n",
      "  Parsing ../Stacks_2/FG010.matches.tsv\n",
      "  Parsing ../Stacks_2/FG011.matches.tsv\n",
      "  Parsing ../Stacks_2/FG013.matches.tsv\n",
      "  Parsing ../Stacks_2/FG014.matches.tsv\n",
      "  Parsing ../Stacks_2/FG015.matches.tsv\n",
      "  Parsing ../Stacks_2/FG016.matches.tsv\n",
      "  Parsing ../Stacks_2/FG017.matches.tsv\n",
      "  Parsing ../Stacks_2/FG018.matches.tsv\n",
      "  Parsing ../Stacks_2/FG019.matches.tsv\n",
      "  Parsing ../Stacks_2/FG021.matches.tsv\n",
      "  Parsing ../Stacks_2/FG024.matches.tsv\n",
      "  Parsing ../Stacks_2/FG026.matches.tsv\n",
      "  Parsing ../Stacks_2/FG027.matches.tsv\n",
      "  Parsing ../Stacks_2/FG031.matches.tsv\n",
      "  Parsing ../Stacks_2/FG032.matches.tsv\n",
      "  Parsing ../Stacks_2/FG033.matches.tsv\n",
      "  Parsing ../Stacks_2/FG034.matches.tsv\n",
      "  Parsing ../Stacks_2/FG035.matches.tsv\n",
      "  Parsing ../Stacks_2/FG100_A.matches.tsv\n",
      "Warning: unable to find any matches in file 'FG100_A', excluding this sample from population analysis.\n",
      "  Parsing ../Stacks_2/FG100_B.matches.tsv\n",
      "  Parsing ../Stacks_2/FG101_A.matches.tsv\n",
      "  Parsing ../Stacks_2/FG101_B.matches.tsv\n",
      "  Parsing ../Stacks_2/FG102_A.matches.tsv\n",
      "  Parsing ../Stacks_2/FG102_B.matches.tsv\n",
      "  Parsing ../Stacks_2/FG103_A.matches.tsv\n",
      "  Parsing ../Stacks_2/FG103_B.matches.tsv\n",
      "  Parsing ../Stacks_2/FG104_A.matches.tsv\n",
      "  Parsing ../Stacks_2/FG104_B.matches.tsv\n",
      "  Parsing ../Stacks_2/FG106.matches.tsv\n",
      "  Parsing ../Stacks_2/FG107.matches.tsv\n",
      "  Parsing ../Stacks_2/FG108.matches.tsv\n",
      "  Parsing ../Stacks_2/FG109.matches.tsv\n",
      "  Parsing ../Stacks_2/FG111.matches.tsv\n",
      "  Parsing ../Stacks_2/FG201.matches.tsv\n",
      "  Parsing ../Stacks_2/FG202.matches.tsv\n",
      "  Parsing ../Stacks_2/FG203.matches.tsv\n",
      "  Parsing ../Stacks_2/FG204.matches.tsv\n",
      "  Parsing ../Stacks_2/FG205.matches.tsv\n",
      "  Parsing ../Stacks_2/FG206.matches.tsv\n",
      "  Parsing ../Stacks_2/FG207.matches.tsv\n",
      "  Parsing ../Stacks_2/FG209.matches.tsv\n",
      "  Parsing ../Stacks_2/FG210.matches.tsv\n",
      "  Parsing ../Stacks_2/FG211.matches.tsv\n",
      "  Parsing ../Stacks_2/FG212.matches.tsv\n",
      "  Parsing ../Stacks_2/Q314.matches.tsv\n",
      "  Parsing ../Stacks_2/Q316.matches.tsv\n",
      "  Parsing ../Stacks_2/Q318.matches.tsv\n",
      "  Parsing ../Stacks_2/Q323.matches.tsv\n",
      "  Parsing ../Stacks_2/Q324.matches.tsv\n",
      "  Parsing ../Stacks_2/Q325.matches.tsv\n",
      "  Parsing ../Stacks_2/Q326.matches.tsv\n",
      "  Parsing ../Stacks_2/Q330.matches.tsv\n",
      "  Parsing ../Stacks_2/Q332.matches.tsv\n",
      "  Parsing ../Stacks_2/Q334.matches.tsv\n",
      "  Parsing ../Stacks_2/Q338.matches.tsv\n",
      "  Parsing ../Stacks_2/Q339.matches.tsv\n",
      "  Parsing ../Stacks_2/Q347.matches.tsv\n",
      "  Parsing ../Stacks_2/Q351.matches.tsv\n",
      "  Parsing ../Stacks_2/Q352.matches.tsv\n",
      "  Parsing ../Stacks_2/Q353.matches.tsv\n",
      "  Parsing ../Stacks_2/Q354.matches.tsv\n",
      "  Parsing ../Stacks_2/Q355.matches.tsv\n",
      "  Parsing ../Stacks_2/Q356.matches.tsv\n",
      "Populating observed haplotypes for 70 samples, 27460 loci.\n",
      "Removed 90 samples from loci that are below the minimum stack depth of 10x\n",
      "Removing 19710 loci that did not pass sample/population constraints... retained 7750 loci.\n",
      "Loading model outputs for 70 samples, 7750 loci.\n",
      "  Parsing ../Stacks_2/FG001.tags.tsv\n",
      "  Parsing ../Stacks_2/FG002.tags.tsv\n",
      "  Parsing ../Stacks_2/FG003.tags.tsv\n",
      "  Parsing ../Stacks_2/FG004.tags.tsv\n",
      "  Parsing ../Stacks_2/FG005.tags.tsv\n",
      "  Parsing ../Stacks_2/FG006.tags.tsv\n",
      "  Parsing ../Stacks_2/FG007.tags.tsv\n",
      "  Parsing ../Stacks_2/FG009.tags.tsv\n",
      "  Parsing ../Stacks_2/FG010.tags.tsv\n",
      "  Parsing ../Stacks_2/FG011.tags.tsv\n",
      "  Parsing ../Stacks_2/FG013.tags.tsv\n",
      "  Parsing ../Stacks_2/FG014.tags.tsv\n",
      "  Parsing ../Stacks_2/FG015.tags.tsv\n",
      "  Parsing ../Stacks_2/FG016.tags.tsv\n",
      "  Parsing ../Stacks_2/FG017.tags.tsv\n",
      "  Parsing ../Stacks_2/FG018.tags.tsv\n",
      "  Parsing ../Stacks_2/FG019.tags.tsv\n",
      "  Parsing ../Stacks_2/FG021.tags.tsv\n",
      "  Parsing ../Stacks_2/FG024.tags.tsv\n",
      "  Parsing ../Stacks_2/FG026.tags.tsv\n",
      "  Parsing ../Stacks_2/FG027.tags.tsv\n",
      "  Parsing ../Stacks_2/FG031.tags.tsv\n",
      "  Parsing ../Stacks_2/FG032.tags.tsv\n",
      "  Parsing ../Stacks_2/FG033.tags.tsv\n",
      "  Parsing ../Stacks_2/FG034.tags.tsv\n",
      "  Parsing ../Stacks_2/FG035.tags.tsv\n",
      "  Parsing ../Stacks_2/FG100_B.tags.tsv\n",
      "  Parsing ../Stacks_2/FG101_A.tags.tsv\n",
      "  Parsing ../Stacks_2/FG101_B.tags.tsv\n",
      "  Parsing ../Stacks_2/FG102_A.tags.tsv\n",
      "  Parsing ../Stacks_2/FG102_B.tags.tsv\n",
      "  Parsing ../Stacks_2/FG103_A.tags.tsv\n",
      "  Parsing ../Stacks_2/FG103_B.tags.tsv\n",
      "  Parsing ../Stacks_2/FG104_A.tags.tsv\n",
      "  Parsing ../Stacks_2/FG104_B.tags.tsv\n",
      "  Parsing ../Stacks_2/FG106.tags.tsv\n",
      "  Parsing ../Stacks_2/FG107.tags.tsv\n",
      "  Parsing ../Stacks_2/FG108.tags.tsv\n",
      "  Parsing ../Stacks_2/FG109.tags.tsv\n",
      "  Parsing ../Stacks_2/FG111.tags.tsv\n",
      "  Parsing ../Stacks_2/FG201.tags.tsv\n",
      "  Parsing ../Stacks_2/FG202.tags.tsv\n",
      "  Parsing ../Stacks_2/FG203.tags.tsv\n",
      "  Parsing ../Stacks_2/FG204.tags.tsv\n",
      "  Parsing ../Stacks_2/FG205.tags.tsv\n",
      "  Parsing ../Stacks_2/FG206.tags.tsv\n",
      "  Parsing ../Stacks_2/FG207.tags.tsv\n",
      "  Parsing ../Stacks_2/FG209.tags.tsv\n",
      "  Parsing ../Stacks_2/FG210.tags.tsv\n",
      "  Parsing ../Stacks_2/FG211.tags.tsv\n",
      "  Parsing ../Stacks_2/FG212.tags.tsv\n",
      "  Parsing ../Stacks_2/Q314.tags.tsv\n",
      "  Parsing ../Stacks_2/Q316.tags.tsv\n",
      "  Parsing ../Stacks_2/Q318.tags.tsv\n",
      "  Parsing ../Stacks_2/Q323.tags.tsv\n",
      "  Parsing ../Stacks_2/Q324.tags.tsv\n",
      "  Parsing ../Stacks_2/Q325.tags.tsv\n",
      "  Parsing ../Stacks_2/Q326.tags.tsv\n",
      "  Parsing ../Stacks_2/Q330.tags.tsv\n",
      "  Parsing ../Stacks_2/Q332.tags.tsv\n",
      "  Parsing ../Stacks_2/Q334.tags.tsv\n",
      "  Parsing ../Stacks_2/Q338.tags.tsv\n",
      "  Parsing ../Stacks_2/Q339.tags.tsv\n",
      "  Parsing ../Stacks_2/Q347.tags.tsv\n",
      "  Parsing ../Stacks_2/Q351.tags.tsv\n",
      "  Parsing ../Stacks_2/Q352.tags.tsv\n",
      "  Parsing ../Stacks_2/Q353.tags.tsv\n",
      "  Parsing ../Stacks_2/Q354.tags.tsv\n",
      "  Parsing ../Stacks_2/Q355.tags.tsv\n",
      "  Parsing ../Stacks_2/Q356.tags.tsv\n",
      "Generating nucleotide-level summary statistics for population 'WA_Strait'\n",
      "Population 'WA_Strait' contained 1123 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'WA_SanJuans'\n",
      "Population 'WA_SanJuans' contained 760 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'WA_Dabob'\n",
      "Population 'WA_Dabob' contained 762 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'AK'\n",
      "Population 'AK' contained 430 incompatible loci -- more than two alleles present.\n",
      "Generating nucleotide-level summary statistics for population 'CA_Catalina'\n",
      "Population 'CA_Catalina' contained 723 incompatible loci -- more than two alleles present.\n",
      "Tallying loci across populations...done.\n",
      "Pruned 2343 variant sites due to filter constraints.\n",
      "  (enable the --verbose flag to record the reason why each site was filtered in the batch_X.populations.log file.)\n",
      "Removing 1 additional loci for which all variant sites were filtered... retained 7749 loci.\n",
      "Regenerating nucleotide-level summary statistics for population 'WA_Strait'\n",
      "Population 'WA_Strait' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'WA_SanJuans'\n",
      "Population 'WA_SanJuans' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'WA_Dabob'\n",
      "Population 'WA_Dabob' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'AK'\n",
      "Population 'AK' contained 0 incompatible loci -- more than two alleles present.\n",
      "Regenerating nucleotide-level summary statistics for population 'CA_Catalina'\n",
      "Population 'CA_Catalina' contained 0 incompatible loci -- more than two alleles present.\n",
      "Re-tallying loci across populations...done.\n",
      "Generating haplotype-level summary statistics for population 'WA_Strait'\n",
      "Generating haplotype-level summary statistics for population 'WA_SanJuans'\n",
      "Generating haplotype-level summary statistics for population 'WA_Dabob'\n",
      "Generating haplotype-level summary statistics for population 'AK'\n",
      "Generating haplotype-level summary statistics for population 'CA_Catalina'\n",
      "Writing 7749 loci to summary statistics file, '../Stacks_2/batch_100.sumstats.tsv'\n",
      "Writing 7749 loci to observed haplotype file, '../Stacks_2/batch_100.haplotypes.tsv'\n",
      "Writing population data to GenePop file '../Stacks_2/batch_100.genepop'\n"
     ]
    }
   ],
   "source": [
    "!stacks populations -b 100 -P ../Stacks_2 -M ../popmap_cragigrun1.txt -t 10 -r .75 -p 5 -m 10 --genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How many loci remained after this second round of ``populations``?\n",
    "\n",
    "Use code:\n",
    "\n",
    "``python count_loci_genepop.py Stacks/batch_100.genepop``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your genepop file has 82265 columns of genotypes.\r\n"
     ]
    }
   ],
   "source": [
    "!python count_loci_genepop.py Stacks_2/batch_100.genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that I've rewritten this script to be more specific (now that I understand how ``populations`` works better!) ... run count again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hgfs/SHARED_FOLDER/Git_repo/CRAGIG_run1/Scripts\n"
     ]
    }
   ],
   "source": [
    "cd /mnt/hgfs/SHARED_FOLDER/Git_repo/CRAGIG_run1/Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Genepop file has 7740 loci with 82265 SNPs.\r\n"
     ]
    }
   ],
   "source": [
    "!python count_snps_loci_genepop.py ../../../WorkingFolder/Stacks_2/batch_100.genepop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Wow. That's an average of 10ish SNPs per tag. Yikes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**20170313**\n",
    "\n",
    "There must be some issue because my genepop file has more than my cleaned catalog made with my \"reference genome,\" so I'll need to go back and check to see where the error is. I've gone through my files and records, and chatted with everyone, and really can't figure it out. So I spent some time writing the scripts for the following steps and familiarizing myself with those steps, here below. I think the wisest next step is to rerun the pipeline, maybe even from the start. Yikes.\n",
    "\n",
    "** 20170314**\n",
    "\n",
    "Issue resolved! It was my misunderstanding. I misunderstood how Genepop files work, and the haplotype vs. locus vs. snp vs. tag thing. Genepop files from ``populations`` output are SNPs, so when I count columns, I'm actually counting SNPs and not loci. So - one major thing to resolve is whether I want to analyze my data by SNP or by locus, and I think I want to do by locus, but conveniently I already moved forward with SNPs yesterday. So after the parts by SNPs, I'll work with by locus. That will involve writing a script to parse the ``populations`` haplotypes file into a tags Genepop instead of SNPSs, and then repeating much of the same steps through.\n",
    "\n",
    "\n",
    "## Post-Stacks filtering & DAPC with SNPs Genepop\n",
    "\n",
    "## Reformat Genepop file to transposed CSV\n",
    "\n",
    "I'm skipping a couple of the steps that most of our lab mates working on finfish use, so I'm jumping straight from ``populations`` round 2 to filter for missing values and then try to make a DAPC.\n",
    "\n",
    "Mary gave me some pro tips on how to make a \"genepop\" file a la Eleni.\n",
    "\n",
    "- make it a CSV\n",
    "- sample names are column headers\n",
    "- loci listed down column 1\n",
    "- genotypes in cells\n",
    "- cell [1,1] says \"sample\"\n",
    "\n",
    "Because I'm skipping some steps, I need to write a script that takes a genepop file out of ``populations`` and reformats it to work with Eleni's scripts.\n",
    "\n",
    "Made script, link to [script here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Scripts/pop_gp_to_csv_gp.py), link to [notebook here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/reformat_populations_genepop_to_genepop-like_CSV.ipynb).\n",
    "\n",
    "Call looked like:\n",
    "\n",
    "``python pop_gp_to_csv_gp.py -i ../../../WorkingFolder/Stacks_2/batch_100.genepop -o newformat.txt``\n",
    "\n",
    "## Filtering for missing values\n",
    "\n",
    "**20170313**\n",
    "\n",
    "Then I wrote a script that takes this transposed genepop file and the population map, and filters for a particular missing value threshold. I  originally planned on using a cut off of .5, but that filtered out none of my loci. So I changed it to be more conservative, to .2, which is what Mary uses. It filtered out 86.5 percent of my loci.\n",
    "\n",
    "The call looked like:\n",
    "\n",
    "```\n",
    "!python filter_missval.py \\\n",
    "-g ../../../WorkingFolder/transposed_genepop.csv \\\n",
    "-p ../../../WorkingFolder/popmap_cragigrun1.txt \\\n",
    "-k ../../../WorkingFolder/Stacks_2/keptloci_20170313.csv \\\n",
    "-l ../../../WorkingFolder/Stacks_2/lostloci_20170313.csv \\\n",
    "-t .2\n",
    "```\n",
    "\n",
    "And the output looked like:\n",
    "``This script filtered out 11035 loci, out of a total of 82264 loci.\n",
    "\n",
    "Your keptloci file should have 71229 loci, and your lostloci file should have 11035 loci.\n",
    "\n",
    "You retained 86.5 percent of your loci after filtering for missing values.``\n",
    "\n",
    "\n",
    "Here's the [notebook](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/filter_for_missing_values.ipynb).\n",
    "\n",
    "And [here's the script](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Scripts/filter_missval.py).\n",
    "\n",
    "\n",
    "## Convert back to Genepop\n",
    "\n",
    "Then I wrote a script that takes these filtered loci and writes them back to a regular genepop file, so I can try to make a DAPC and process it in Genepop for HWE.\n",
    "\n",
    "Here's the [notebook](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/transposed_CSV_back_to_genepop.ipynb)\n",
    "and here's the [script](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Scripts/transpo_csv_to_genepop.py).\n",
    "\n",
    "## DAPC\n",
    "\n",
    "**20170313**\n",
    "\n",
    "I ran a DAPC script from Charlie and Eleni to make a preliminary plot. The script lives [here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Scripts/CRAGIG_RUN1_DAPC_20170313.R) and the plot looked like this:\n",
    "\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/dapc_20170313.png?raw=true)\n",
    "\n",
    "Where:\n",
    "- WA1 = Sekiu\n",
    "- WA2 = San Juans\n",
    "- WA3 = Dabob\n",
    "\n",
    "\n",
    "\n",
    "## Second DAPC, after filter for MAF\n",
    "**20170322**\n",
    "\n",
    "Now, do the same except filter for minor allele frequency of .05 across populations.\n",
    "\n",
    "### Filter for MAF\n",
    "\n",
    "I wrote a quick script to filter for MAF, but not at the population level. I have only 7 individuals for AK, and am having trouble understanding why filter for MAF at the population level. So while I wait, I wanted to try this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Convert back to Genepop\n",
    "\n",
    "### DAPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tags Genepop Analysis\n",
    "\n",
    "**20170314**\n",
    "\n",
    "I went through and made a preliminary DAPC with my SNPs Genepop (output from ``populations``), and now I need to make a script that parses the haplotypes file into a tags Genepop.\n",
    "\n",
    "**20170315**\n",
    "\n",
    "Made the script! Notebook [here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/parse_haplotypesfile_to_tagsGenepop.ipynb), script [here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Scripts/parse_hapfile_to_tagGenePop.py).\n",
    "\n",
    "For this run, the file's name is ``tagsGenepop_20170314.genepop``\n",
    "\n",
    "I was staring at it in my text editor and realized that my sequencing replicates are actually quite different, which is pretty disturbing. So, now I'm working to write an algorithm for quantify genotyping error. The error could be high because of contamination, or it could be high because of something inherent to how genetically variable shellfish are + sequencing depth and error, etc. So some next steps (suggested by Brent) are to test for contamination by seeing if samples that I extracted tissue from on the same day are more similar to each other. Something else would be to see if genotypes with more than two alleles happen within tags or randomly across tags. Within tags would suggest paralogous loci, across tags may be evidence of contamination.\n",
    "\n",
    "**20170316**\n",
    "\n",
    "I worked out genotyping error between SNP genotypes and haplotype genotypes in a couple of workbooks, and found that the former to be negligibile (99.6% perfect match between biological replicates) and the latter to be huge (~25% match between biological replicates). [SNP genotyping error notebook here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/data_processing_notebooks/quantifying_genotyping_error_in_snpsGP.ipynb), [haplotype genotyping error notebook here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/data_processing_notebooks/quantifying_genotyping_error_in_tagsGP.ipynb).\n",
    "\n",
    "I also made this plot ([notebook here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/data_processing_notebooks/Plot_incidence_genotypes_morethantwoalleles_from_haplotypesfile.ipynb)) to see whether there were few tags that had a ton of genotypes with more than two alleles, expecting to see some pattern supporting paralogous genes. But! I did not, generally, except for this one weird bar of higher values. To be extra clear, frequency here means number of individual genotypes at a tag that had more than two alleles in the genotype (as if polyploid, or paralogous gene).\n",
    "\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/freq_3ormore_haps_in_genotype_20170315.png?raw=true)\n",
    "\n",
    "So! Next steps after much chatter.\n",
    "\n",
    "1. make a scatter plot of incidince of multiple alleles at a single genotype within a locus, against number of SNPs at that locus - because I think the more SNPs there are, the more likely a  small sequencing error can result in a huge haplotype call error.\n",
    "\n",
    "2. look at number snps in catalogs file, see if matches patterns in haplotypes file re: number of alleles at a genotype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Num SNPs in haplotype versus frequency of individual genotypes with more than two alleles\n",
    "\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/haplength_byinc_morethantwoalleles_20170316.png?raw=true)\n",
    "\n",
    "Correlation, R = .31, so not huge at all. Not what I expected! I'm going to take a closer look at the catalog SNPs files to try and understand better the number of alleles per genotype, and the mismatch in snp and haplotype genotyping error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Number of individual genotypes with different numbers of alleles\n",
    "\n",
    "**20170317**\n",
    "\n",
    "* Percent of genotypes with missing values: 13.12%\n",
    "* Percent of genotypes with diploid genotypes: 76.28%\n",
    "* Percent of genotypes with polyploid genotypes: 10.59%\n",
    "    \n",
    "And, histogram of number of additional alleles (additional to the two in a diploid genotype):\n",
    "\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/images_for_notebooks/hist_numalleles_indgen_haps_20170317.png?raw=true)\n",
    "\n",
    "Notebook [here](http://localhost:8888/notebooks/Git_repo/CRAGIG_run1/Notebooks/data_processing_notebooks/Plot_incidence_genotypes_morethantwoalleles_from_haplotypesfile.ipynb).\n",
    "\n",
    "It would also be nice to know what the average allele frequency is of these additional alleles. If it tends to be below a certain threshold, maybe we can convince ourselves that it's mostly sequencing error and worth throwing out. How to get that information though... I was also thinking that if the issue is that a small amount of sequencing error can lead to a huge genotyping error, then I should probably filter for some threshold, like a haplotype can only be kept if it exists in at least two individuals. For individuals with three alleles at a single genotype, I'd be curious to know the frequency of the minor allele. That would help me inform how to best filter haplotypes. \n",
    "\n",
    "## Rank decreasing allele frequencies for genotypes with N alleles, within an individual\n",
    "\n",
    "**20170318**\n",
    "\n",
    "I decided to explore the individual alleles files to better understand the minor allele frequencies, [notebook here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/data_processing_notebooks/exploring_ind_alleles_files.ipynb). I took an individual .alleles file (from Scallop FG001) and pulled out just those tags with 2,3,4, and 5 alleles respectively. Then I extracted the allele frequency, put them in rank decreasing order, and plotted them. I did this to help me understand whether these additional alleles are really uncommon, or if they're pretty evenly distributed. Here are the plots:\n",
    "\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/rank_afs_2alls_20170318.png?raw=true)\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/rank_afs_3alls_20170318.png?raw=true)\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/rank_afs_4alls_20170318.png?raw=true)\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/rank_afs_5alls_20170318.png?raw=true)\n",
    "\n",
    "It would be pretty cool to be able to do this across individuals - maybe try writing a script that collects allele frequency data from a set of samples.\n",
    "\n",
    "## Making same rank decreasing AF plots, but using genotypes from multiple individuals\n",
    "\n",
    "**20170318**\n",
    "\n",
    "I wrote a script that allows me to do the same, but incorporate genotypes from multiple individuals. Here, I included FG001, FG002, and FG003. [Notebook here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/script_demos_notebooks/plot_rank_dec_AFs_from_set_individuals.ipynb), [script here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Scripts/plot_dec_rank_AFs_across_indvs.py).\n",
    "\n",
    "And the plots:\n",
    "\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/images_for_notebooks/rank_dec_afs_w_2_alls.png?raw=true)\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/images_for_notebooks/rank_dec_afs_w_3_alls.png?raw=true)\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/images_for_notebooks/rank_dec_afs_w_4_alls.png?raw=true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I want to see whether the number of genotypes at the SNP level with more than two alleles was similar to that of the haplotype level, but I'm finding it pretty hard to get that SNP data out of the individual .snps, .tags, and .alleles files. \n",
    "\n",
    "In the meantime, I can make Fis plots from the SNP Genepop file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Fis by population plots\n",
    "\n",
    "**20170318**\n",
    "\n",
    "From [notebook here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Notebooks/data_processing_notebooks/Fis_from_SNP_Genepop.ipynb), I made plots of Fis by population using the SNP genepop file. **Note** that about half of the Fis estimates were NAs, and I'm not sure what that means about the data.\n",
    "\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/Fis_WA_Strait_20170317.png?raw=true)\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/Fis_WA_SanJuans_20170317.png?raw=true)\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/Fis_WA_Dabob_20170317.png?raw=true)\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/Fis_CA_20170317.png?raw=true)\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/Fis_AK_20170317.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Distribution of position of SNP in tag\n",
    "\n",
    "I just got off the phone with a peer who says that they see a higher abundance of SNPs toward the end of the tag, causing them to believe that Stacks may be calling SNPs that aren't really SNPs. This is after they use FASTQC and a quality filter cut off, just like I used. It would be worth it to see the distribution of positin of SNPs in tags.\n",
    "\n",
    "I just wrote a brief script to plot this distribution from the SNP Genepop file that comes out of ``populations``. Script is [here](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Scripts/dist_SNPpos_intags.py)!\n",
    "\n",
    "And here's the plot it made:\n",
    "\n",
    "![image](https://github.com/nclowell/RAD_Scallops/blob/master/CRAGIG_run1/Analyses/snp_position_in_tag_20170322.png?raw=true)\n",
    "\n",
    "It doesn't look like there are more SNPs toward the end of the tag, which doesn't make me feel any more dubious about SNPs later in the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
